{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Dicom meta data to MasterKey\n",
    "\n",
    "<details>\n",
    "<summary>STEP 1 BIG PICTURE</summary>\n",
    "We collected data from centers in folders, named as patient ID (e.g. admission). We want to clean these directories, so \n",
    "I: Each CT study is placed in one folder\n",
    "II: Store cases in an excel file, with its dicom files in the table, and all other variables (outcome, clinical, pathology data) stored here. We call this master key, which also contains patient id (un-anonymized) along with the key for anonymization.\n",
    "III: Transfer dicom-pnly files to new destination and anonymize these images.\n",
    "</details>\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>PREVIOUS STEP</summary>\n",
    "We find all file types in our directory (I ran the code for each center sepratly. Having 1.5 terabytes of informaiton and ~1800 cases, it collectivly took 30 hours on a RTX3080Ti labtob and Corei912gen and 32Ram)\n",
    "</details>\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>THIS STEP</summary>\n",
    "In this step we will add unique dicom meta data about patient info, study info, and series info\n",
    "</details>\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>NEXT STEP</summary>\n",
    "Finding dicom meta data\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINAL 20231216\n",
    "#My context: I coded this on my windows11 with RTC3080Ti and Corei9-12gen and 32G Ram. I am coding on VS code and using jupyter notebook.\n",
    "#Your requirment: It doesn't need any exceptional hardward you can run it on an average pc/labtob\n",
    "\n",
    "import pydicom as pm #for reading dicoms\n",
    "import os #for looping through system direcotries\n",
    "from pydicom.multival import MultiValue #for reading dicom metadata\n",
    "from pydicom.valuerep import PersonName #since tunring dictionary to json raised an error you should use this\n",
    "from tqdm.notebook import tqdm #for that fancy loop progress, I like it though\n",
    "import pandas as pd #for tunring dic to excel, first we trasnform it to pandas dataframe\n",
    "import json #for storing as json\n",
    "\n",
    "from IPython.display import HTML #so you can click on the sotred excel and json and open it from jupyter notebook\n",
    "\n",
    "def get_dicom_tag_value(dicom_file, tag, default=None):\n",
    "    '''this function will get the dicom tag from the dicom filde for the given tag/code'''\n",
    "    tag_value = dicom_file.get(tag, None)\n",
    "    if tag_value is None:\n",
    "        return default\n",
    "    if isinstance(tag_value, MultiValue):\n",
    "        return list(tag_value)  # Convert MultiValue to list\n",
    "    return tag_value.value\n",
    "\n",
    "def get_path_to_first_subfolder(full_path, first_subfolder):\n",
    "    \"\"\"this will get the path to the first folder of root, which is the subfolder that contains all dicom filed of one dicom study \"\"\"\n",
    "    path_parts = full_path.split(os.sep)\n",
    "    if first_subfolder in path_parts:\n",
    "        subfolder_index = path_parts.index(first_subfolder)\n",
    "        return os.sep.join(path_parts[:subfolder_index + 1])\n",
    "    else:\n",
    "        return full_path\n",
    "\n",
    "def count_subfolders(directory):\n",
    "    '''this will cont the number of files and folders within a direcotyr'''\n",
    "    total_subfolders = 0\n",
    "    total_files=0\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        total_subfolders += len(dirs)\n",
    "        total_files += len(files)\n",
    "    return total_subfolders,total_files \n",
    "\n",
    "\n",
    "class CustomJSONEncoder(json.JSONEncoder): #this class will turn our multilevel dictionary into a json file\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, MultiValue):\n",
    "            return list(obj)  # Convert MultiValue to list\n",
    "        elif isinstance(obj, PersonName):\n",
    "            return str(obj)   # Convert PersonName to string\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "def ensure_json_extension(directory): \n",
    "    '''this function will ensure that definied json direcotry contains the required extension, otherwise, it will add this to the end of definied dir'''\n",
    "    if not directory.endswith(\".json\"):\n",
    "        return directory + \"\\\\JSON.json\"\n",
    "    return directory\n",
    "\n",
    "def ensure_excel_extension(directory):\n",
    "    '''this function will ensure that definied excel direcotry contains the required extension, otherwise, it will add this to the end of definied dir'''\n",
    "    if not directory.endswith(\".xlsx\"):\n",
    "        return directory + \"\\\\excel.xlsx\"\n",
    "    return directory\n",
    "\n",
    "def create_clickable_dir_path(dir_path):\n",
    "    # Convert the directory path to a file URL\n",
    "    file_url = f\"{dir_path}\"\n",
    "    return HTML(f'<a href=\"{file_url}\" target=\"_blank\">{dir_path}</a>')\n",
    "\n",
    "\n",
    "\n",
    "def get_dicomdir_give_dicomdicom_datadic(dicom_dir, #direcotry that you want to read, usually dicom studies should be in one folder, preferably with patient unique id/name\n",
    "                                     dicom_validation=True, #this will check wether the file in the loop is dicom or not. Although make it slower, I recommend using it to ensure only dicom files go through loop \n",
    "                                     folder_list_name_indicomdir=None, #In your dicom_dir you can include list of folders name that you want to read. It will not read other folders. Kepp in mind that this will look into subfolders in the main folder, and not the subfolders of subfolders :)\n",
    "                                     store_as_json_dir=None, #if you want to store your ditionary as json, give your desired json direcotry\n",
    "                                     store_as_excel_dir=None #if you want to store your ditionary as excel, give your desired excel direcotry\n",
    "                                     ):\n",
    "    \"\"\"\n",
    "    This function creates a multi-level dictionary for DICOM meta data (named dicom_data) in a directory (named dicom_dir).\n",
    "    The top level has the last component of dicom_dir, which is the first level subfolder, as a key.\n",
    "    For each subforled it will store study data within this dic, along with another dicitonary for series data, within this study dictionary.\n",
    "    For series dictionary the data corresponding for series number will be stored.\n",
    "    We also have another private_info dictionary within subfodler dictionary.\n",
    "    \n",
    "    - dicom_validation: If you set dicom_validation=True, it will validate the file in the loop for being an dicom file. This is super important although it makes code slower.\n",
    "    Becaouse, sometimes some dicom files have no extension, and also reading other files may cause error in the loop.\n",
    "    \n",
    "    - folder_list_name_indicomdir: #In your dicom_dir you can include list of folders name that you want to read. It will not read other folders. Kepp in mind that this will look into subfolders in the main folder, and not the subfolders of subfolders :)\n",
    "    \n",
    "    - store_as_json_dir: if you want to store your ditionary as json, give your desired json direcotry\n",
    "    \n",
    "    - store_as_excel_dir: if you want to store your ditionary as excel, give your desired excel direcotry\n",
    "    \n",
    "    For using this function, the best practice is to place each folder containing one dicom study in subfolder, under the dicom_dir. \n",
    "    However, you can change finding unique dicom studies, even placed next to each other beacouse I definied the study_unique=f'{first_subfolder}_{study_id}_{study_date}'.\n",
    "    If you want your code to be faster you can chane the study_unique to study_unique=first_subfolder. It makes your code 15% faster, sometimes at the cost of incurrect retrival.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    total_subfolder,total_files=count_subfolders(dicom_dir)\n",
    "    print(f'your direcotry contains {total_subfolder} folders and {total_files} files')\n",
    "    \n",
    "    last_dir_name = os.path.basename(os.path.normpath(dicom_dir))\n",
    "    dicom_data = {last_dir_name: {}}\n",
    "\n",
    "    for root, dirs, files in tqdm(os.walk(dicom_dir), desc=\"Processing directories\", total=total_subfolder,unit='folder'):\n",
    "        if folder_list_name_indicomdir:\n",
    "            split_path = root.replace(dicom_dir, '').split(os.sep)\n",
    "            first_subfolder = split_path[1] if len(split_path) > 1 else \"\"\n",
    "            if first_subfolder not in folder_list_name_indicomdir:\n",
    "                print(f\"\"\"The folder {first_subfolder} was not in your definied list.\"\"\")\n",
    "                continue  # Skip if the first subfolder is not in the user-defined list\n",
    "            \n",
    "        for file in files:\n",
    "            if dicom_validation and not pm.misc.is_dicom(os.path.join(root, file)):\n",
    "                continue # Skip if the it is not dicom file\n",
    "                   \n",
    "\n",
    "            try:\n",
    "                dicom_file = pm.dcmread(os.path.join(root, file))\n",
    "                study_id = get_dicom_tag_value(dicom_file, (0x0020, 0x0010))\n",
    "                dicom_data_number = get_dicom_tag_value(dicom_file, (0x0020, 0x0011))\n",
    "                study_date = get_dicom_tag_value(dicom_file, (0x0008, 0x0020))\n",
    "                split_path = root.replace(dicom_dir, '').split(os.sep)\n",
    "                first_subfolder = split_path[1] if len(split_path) > 1 else \"\"\n",
    "                if study_id and dicom_data_number and study_date:\n",
    "                    study_unique = f'{first_subfolder}_{study_id}_{study_date}' #you can change it for increasing the speed > study_unique=first_subfolder\n",
    "                    if study_unique not in dicom_data[last_dir_name]:\n",
    "                        private_info={'name': get_dicom_tag_value(dicom_file, (0x0010, 0x0010)),\n",
    "                                      'institute': get_dicom_tag_value(dicom_file, (0x0008, 0x0080)),\n",
    "                                      'patient_id': get_dicom_tag_value(dicom_file, (0x0010, 0x0020)),\n",
    "                                      'accession_number':get_dicom_tag_value(dicom_file, (0x0008, 0x0050))\n",
    "                                      }\n",
    "                        \n",
    "                        dicom_data[last_dir_name][study_unique] = {\n",
    "                            'dir_to_root': get_path_to_first_subfolder(root, first_subfolder),\n",
    "                            'study_description': get_dicom_tag_value(dicom_file, (0x0008, 0x1030)),\n",
    "                            'date': study_date,\n",
    "                            'age': get_dicom_tag_value(dicom_file, (0x0010, 0x1010)),\n",
    "                            'sex': get_dicom_tag_value(dicom_file, (0x0010, 0x0040)),\n",
    "                            'manufacture_model': get_dicom_tag_value(dicom_file, (0x0008, 0x1090)),\n",
    "                            'manufacture_brand': get_dicom_tag_value(dicom_file, (0x0008, 0x0070)),\n",
    "                            'manufacture_brand': get_dicom_tag_value(dicom_file, (0x0008, 0x0070)),\n",
    "                            'protocol': get_dicom_tag_value(dicom_file, (0x0018, 0x1030)),\n",
    "                            'study_id': study_id,\n",
    "                            'patient_weight': get_dicom_tag_value(dicom_file, (0x0010, 0x1030)),\n",
    "                            'Image_type': get_dicom_tag_value(dicom_file, (0x0008, 0x0008)),\n",
    "                            'body_part': get_dicom_tag_value(dicom_file, (0x0018, 0x0015)),\n",
    "                            'modalitty':get_dicom_tag_value(dicom_file, (0x0008, 0x0050)),\n",
    "                            'private_info':private_info,\n",
    "                            'image_dicom_data_list': {}\n",
    "                        }\n",
    "\n",
    "                    \n",
    "\n",
    "                    dicom_data_info = {\n",
    "                        'dicom_data_description': get_dicom_tag_value(dicom_file, (0x0008, 0x103E)),\n",
    "                        'body_part': get_dicom_tag_value(dicom_file, (0x0018, 0x0015)),\n",
    "                        'slice_thickness': get_dicom_tag_value(dicom_file, (0x0018, 0x0050)),\n",
    "                        'Image_comment': get_dicom_tag_value(dicom_file, (0x0020, 0x4000)),\n",
    "                        'kvp': get_dicom_tag_value(dicom_file, (0x0018, 0x0060)),\n",
    "                        'exposure': get_dicom_tag_value(dicom_file, (0x0018, 0x1152)),\n",
    "                        'exposure_time': get_dicom_tag_value(dicom_file, (0x0018, 0x1150)),\n",
    "                    }\n",
    "                    dicom_data[last_dir_name][study_unique]['image_dicom_data_list'][dicom_data_number] = dicom_data_info\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\"\"Error reading for {file}::: {e} \\n \"\"\")\n",
    "                continue\n",
    "            \n",
    "    if store_as_json_dir is not None:\n",
    "        try:\n",
    "            json_read = json.dumps(dicom_data, indent=4, cls=CustomJSONEncoder)\n",
    "            store_as_json_dir=str(store_as_json_dir)\n",
    "            store_as_json_dir=ensure_json_extension(store_as_json_dir)\n",
    "            with open(store_as_json_dir, 'w') as json_file:\n",
    "                json_file.write(json_read)\n",
    "            print(f\"\"\"Json stored at :::\"\"\")\n",
    "            display(create_clickable_dir_path(store_as_json_dir))         \n",
    "        except:\n",
    "            print(f\"\"\"Error storing the json ::: {e} \\n \"\"\")\n",
    "            \n",
    "    if store_as_excel_dir is not None:\n",
    "        try:\n",
    "            dataframes = []\n",
    "            for key, value in dicom_data.items():\n",
    "                # Convert value to DataFrame if necessary\n",
    "                df = pd.DataFrame(value)\n",
    "                # Add the key as a new column or as part of the index\n",
    "                df['Key'] = key  # Add key as a column\n",
    "                # df = df.set_index(['Key'], append=True)  # Add key as part of a MultiIndex\n",
    "                dataframes.append(df)\n",
    "\n",
    "            # Concatenate all dataframes\n",
    "            df2 = pd.concat(dataframes).T\n",
    "            store_as_excel_dir=str(store_as_excel_dir)\n",
    "            store_as_excel_dir=ensure_excel_extension(store_as_excel_dir)\n",
    "            df2.to_excel(store_as_excel_dir)\n",
    "            print(f\"\"\"Excel stored at :::\"\"\")\n",
    "            display(create_clickable_dir_path(store_as_excel_dir))          \n",
    "        except:\n",
    "            print(f\"\"\"Error storing the excel ::: {e} \\n \"\"\")\n",
    "            \n",
    "                                 \n",
    "    return dicom_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO\n",
    "\n",
    "# r\"F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\Shiraz\\Valid Case Image\"                   #Done  #corropted json NEED TO BE DONE AGAIN\n",
    "\n",
    "# r'F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\Taleghani\\Maybe Case Image',               #Done #corropted json NEED TO BE DONE AGAIN\n",
    "# r'F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\Taleghani\\Maybe Control Image'             #Done\n",
    "\n",
    "# r\"F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\Dr Radmard\\Valid Case\"                     #Done\n",
    "\n",
    "#r\"F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\Emam Kh\\Maybe Case Image\"                   #in progress\n",
    "#r\"F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\Emam Kh\\Maybe Control (wo rep)\"             #in progress\n",
    "#r\"F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\Emam Kh\\Maybe Control Image\"                #in progress\n",
    "#r\"F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\Emam Kh\\Valid Case\"                         #in progress\n",
    "\n",
    "#r\"F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\Guilan\\Valid Case\"                           #in progress \n",
    "#r\"F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\Guilan\\Valid Control\"                        #in progress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicom_dir=r\"F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\Dr Radmard\\Valid Case\" \n",
    "save_dir_json=r'F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\Radmard_all_dcm.json'\n",
    "save_dir_xlsx=r'F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\Radmard_all_dcm.xlsx'\n",
    "\n",
    "\n",
    "dicom_dic=get_dicomdir_give_dicomdicom_datadic(\n",
    "    dicom_dir, #direcotry that you want to read, usually dicom studies should be in one folder, preferably with patient unique id/name\n",
    "                                     dicom_validation=True, #this will check wether the file in the loop is dicom or not. Although make it slower, I recommend using it to ensure only dicom files go through loop \n",
    "                                     folder_list_name_indicomdir=None, #In your dicom_dir you can include list of folders name that you want to read. It will not read other folders. Kepp in mind that this will look into subfolders in the main folder, and not the subfolders of subfolders :)\n",
    "                                     store_as_json_dir=save_dir_json, #if you want to store your ditionary as json, give your desired json direcotry\n",
    "                                     store_as_excel_dir=save_dir_xlsx #if you want to store your ditionary as excel, give your desired excel direcotry\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicom_dir=r\"F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\Guilan\\Valid Control\" \n",
    "save_dir_json=r'F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\Guilan_control_all_dcm.json'\n",
    "save_dir_xlsx=r'F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\Guilan_control_all_dcm.xlsx'\n",
    "\n",
    "\n",
    "dicom_dic=get_dicomdir_give_dicomdicom_datadic(\n",
    "    dicom_dir, #direcotry that you want to read, usually dicom studies should be in one folder, preferably with patient unique id/name\n",
    "                                     dicom_validation=True, #this will check wether the file in the loop is dicom or not. Although make it slower, I recommend using it to ensure only dicom files go through loop \n",
    "                                     folder_list_name_indicomdir=None, #In your dicom_dir you can include list of folders name that you want to read. It will not read other folders. Kepp in mind that this will look into subfolders in the main folder, and not the subfolders of subfolders :)\n",
    "                                     store_as_json_dir=save_dir_json, #if you want to store your ditionary as json, give your desired json direcotry\n",
    "                                     store_as_excel_dir=save_dir_xlsx #if you want to store your ditionary as excel, give your desired excel direcotry\n",
    "                                     )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicom_dir=r\"F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\Guilan\\Valid Case\"\n",
    "save_dir_json=r'F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\Guilan_case_all_dcm.json'\n",
    "save_dir_xlsx=r'F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\Guilan_case_all_dcm.xlsx'\n",
    "\n",
    "\n",
    "dicom_dic=get_dicomdir_give_dicomdicom_datadic(\n",
    "    dicom_dir, #direcotry that you want to read, usually dicom studies should be in one folder, preferably with patient unique id/name\n",
    "                                     dicom_validation=True, #this will check wether the file in the loop is dicom or not. Although make it slower, I recommend using it to ensure only dicom files go through loop \n",
    "                                     folder_list_name_indicomdir=None, #In your dicom_dir you can include list of folders name that you want to read. It will not read other folders. Kepp in mind that this will look into subfolders in the main folder, and not the subfolders of subfolders :)\n",
    "                                     store_as_json_dir=save_dir_json, #if you want to store your ditionary as json, give your desired json direcotry\n",
    "                                     store_as_excel_dir=save_dir_xlsx #if you want to store your ditionary as excel, give your desired excel direcotry\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicom_dir=r\"F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\Emam Kh\\Maybe Case Image\"\n",
    "save_dir_json=r'F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\EmamKh_maybeCase_all_dcm.json'\n",
    "save_dir_xlsx=r'F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\EmamKh_maybeCase_all_dcm.xlsx'\n",
    "\n",
    "dicom_dic=get_dicomdir_give_dicomdicom_datadic(\n",
    "    dicom_dir, #direcotry that you want to read, usually dicom studies should be in one folder, preferably with patient unique id/name\n",
    "                                     dicom_validation=True, #this will check wether the file in the loop is dicom or not. Although make it slower, I recommend using it to ensure only dicom files go through loop \n",
    "                                     folder_list_name_indicomdir=None, #In your dicom_dir you can include list of folders name that you want to read. It will not read other folders. Kepp in mind that this will look into subfolders in the main folder, and not the subfolders of subfolders :)\n",
    "                                     store_as_json_dir=save_dir_json, #if you want to store your ditionary as json, give your desired json direcotry\n",
    "                                     store_as_excel_dir=save_dir_xlsx #if you want to store your ditionary as excel, give your desired excel direcotry\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicom_dir=r\"F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\Emam Kh\\Maybe Control (wo rep)\"\n",
    "save_dir_json=r'F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\EmamKh_maybeControlwithoutrep_all_dcm.json'\n",
    "save_dir_xlsx=r'F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\EmamKh_maybeControlwithoutrep_all_dcm.xlsx'\n",
    "\n",
    "dicom_dic=get_dicomdir_give_dicomdicom_datadic(\n",
    "    dicom_dir, #direcotry that you want to read, usually dicom studies should be in one folder, preferably with patient unique id/name\n",
    "                                     dicom_validation=True, #this will check wether the file in the loop is dicom or not. Although make it slower, I recommend using it to ensure only dicom files go through loop \n",
    "                                     folder_list_name_indicomdir=None, #In your dicom_dir you can include list of folders name that you want to read. It will not read other folders. Kepp in mind that this will look into subfolders in the main folder, and not the subfolders of subfolders :)\n",
    "                                     store_as_json_dir=save_dir_json, #if you want to store your ditionary as json, give your desired json direcotry\n",
    "                                     store_as_excel_dir=save_dir_xlsx #if you want to store your ditionary as excel, give your desired excel direcotry\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicom_dir=r\"F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\Emam Kh\\Maybe Control Image\"\n",
    "save_dir_json=r'F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\EmamKh_maybecontrol_all_dcm.json'\n",
    "save_dir_xlsx=r'F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\EmamKh_maybecontrol_all_dcm.xlsx'\n",
    "\n",
    "dicom_dic=get_dicomdir_give_dicomdicom_datadic(\n",
    "    dicom_dir, #direcotry that you want to read, usually dicom studies should be in one folder, preferably with patient unique id/name\n",
    "                                     dicom_validation=True, #this will check wether the file in the loop is dicom or not. Although make it slower, I recommend using it to ensure only dicom files go through loop \n",
    "                                     folder_list_name_indicomdir=None, #In your dicom_dir you can include list of folders name that you want to read. It will not read other folders. Kepp in mind that this will look into subfolders in the main folder, and not the subfolders of subfolders :)\n",
    "                                     store_as_json_dir=save_dir_json, #if you want to store your ditionary as json, give your desired json direcotry\n",
    "                                     store_as_excel_dir=save_dir_xlsx #if you want to store your ditionary as excel, give your desired excel direcotry\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicom_dir=r\"F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\Emam Kh\\Valid Case\"\n",
    "save_dir_json=r'F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\EmamKh_validcase_all_dcm.json'\n",
    "save_dir_xlsx=r'F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\EmamKh_validcase_all_dcm.xlsx'\n",
    "\n",
    "dicom_dic=get_dicomdir_give_dicomdicom_datadic(\n",
    "    dicom_dir, #direcotry that you want to read, usually dicom studies should be in one folder, preferably with patient unique id/name\n",
    "                                     dicom_validation=True, #this will check wether the file in the loop is dicom or not. Although make it slower, I recommend using it to ensure only dicom files go through loop \n",
    "                                     folder_list_name_indicomdir=None, #In your dicom_dir you can include list of folders name that you want to read. It will not read other folders. Kepp in mind that this will look into subfolders in the main folder, and not the subfolders of subfolders :)\n",
    "                                     store_as_json_dir=save_dir_json, #if you want to store your ditionary as json, give your desired json direcotry\n",
    "                                     store_as_excel_dir=save_dir_xlsx #if you want to store your ditionary as excel, give your desired excel direcotry\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#save to dataframe\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'series' is your dictionary containing the data\n",
    "dataframes = []\n",
    "for key, value in series.items():\n",
    "    # Convert value to DataFrame if necessary\n",
    "    df = pd.DataFrame(value)\n",
    "    # Add the key as a new column or as part of the index\n",
    "    df['Key'] = key  # Add key as a column\n",
    "    # df = df.set_index(['Key'], append=True)  # Add key as part of a MultiIndex\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Concatenate all dataframes\n",
    "df2 = pd.concat(dataframes).T\n",
    "df2.to_excel('F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\Shiraz_Case_dcm.xlsx')\n",
    "\n",
    "json_read = json.dumps(series, indent=4, cls=CustomJSONEncoder)\n",
    "\n",
    "with open(r'F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\Taleghani_MaybeControl_all_dcm.json', 'w') as json_file:\n",
    "    json_file.dump(json_read)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARCHIVED CODES (TRASH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data['Full_Directory'])):\n",
    "    i=0\n",
    "\n",
    "    if data['If_dicom'] is True:\n",
    "        dcm_dir=\"{}\\{}\".format(data[\"Full_Directory\"].iloc[0],data[\"File\"].iloc[0])\n",
    "        dcm_dir\n",
    "        \n",
    "    else:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydicom as pm\n",
    "\n",
    "dcm_dir=\"{}\\{}\".format(data[\"Full_Directory\"].iloc[0],data[\"File\"].iloc[0])\n",
    "dicom_file = pm.dcmread(dcm_dir)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hospital_name= \"Guilan\"\n",
    "directory_shortlist=f\"D:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\{Hospital_name}_data_short_just_dcm.xlsx\"\n",
    "directory_longlist=f\"D:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\{Hospital_name}_data.csv\"\n",
    "\n",
    "\n",
    "\n",
    "directory_longlist=pd.read_csv(directory_longlist)\n",
    "directory_longlist_dcm=directory_longlist[directory_longlist['If_dicom']==True]\n",
    "\n",
    "directory_longlist_dcm=directory_longlist[directory_longlist['If_dicom']==True]\n",
    "directory_longlist_dcm=directory_longlist_dcm.reset_index()\n",
    "\n",
    "\n",
    "\n",
    "i=1\n",
    "dir_path = os.path.join(directory_longlist_dcm.iloc[i][3], directory_longlist_dcm.iloc[i][2])\n",
    "dir_path\n",
    "\n",
    "\n",
    "\n",
    "# this code aimed to get all dicom meta data so I can work with them, and know them, especially for anonymization and knowing the phase.\n",
    "# however, it failed due to different dicom formats. I will use json instead (I should have use it at first place).\n",
    "# the json also has many erorrs, so I added the try-except into the loop to handle erorrs while finishing the loop.\n",
    "\n",
    "dcminfo_list = []  # List to store the individual DataFrame pieces\n",
    "\n",
    "print(f'total rows in your dataframe is {len(directory_longlist_dcm)}')\n",
    "start_time=time.time()\n",
    "\n",
    "for i in range(1, len(directory_longlist_dcm)):\n",
    "    dir_path = os.path.join(directory_longlist_dcm.iloc[i][3], directory_longlist_dcm.iloc[i][2])\n",
    "    \n",
    "    try:\n",
    "        ds = pm.dcmread(dir_path)\n",
    "        ds = pd.DataFrame(ds.values())\n",
    "        if ds.shape[1]>1:\n",
    "            ds= pd.DataFrame({'WARNING_MORETHAN1ROW_DF2CELL': [ds.to_string()]})\n",
    "        else: \n",
    "            ds[0] = ds[0].apply(lambda x: pm.dataelem.DataElement_from_raw(x) if isinstance(x, pm.dataelem.RawDataElement) else x)\n",
    "            ds['name'] = ds[0].apply(lambda x: x.name)\n",
    "            ds['value'] = ds[0].apply(lambda x: x.value)\n",
    "            ds = ds[['name', 'value']]\n",
    "            ds = ds.T\n",
    "            new_header = ds.iloc[0]  # First row as header\n",
    "            ds = ds[1:]  # Taking the rest of the data\n",
    "            ds.columns = new_header  # Setting the new header\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            percentage = (i / len(directory_longlist_dcm)) * 100\n",
    "            end_time = time.time() \n",
    "            elapsed_time = end_time - start_time\n",
    "            print(f'Processed {i} rows, which is {percentage:.2f}% of total rows in {elapsed_time} seconds.')\n",
    "\n",
    "        ds['to_directory'] = dir_path\n",
    "        ds['key2csv']=directory_longlist_dcm['Unnamed: 0'][i]\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_message = str(e)\n",
    "        ds = pd.DataFrame({'WARNING_ERROR': [error_message], 'to_directory': dir_path, 'key2csv': directory_longlist_dcm['Unnamed: 0'][i]})\n",
    "\n",
    "    dcminfo_list.append(ds)\n",
    "\n",
    "for df in dcminfo_list:\n",
    "    rename_duplicate_columns(df)\n",
    "\n",
    "\n",
    "dcminfo_all=pd.concat(dcminfo_list, ignore_index=True, sort=False)\n",
    "dcminfo_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ds = pm.dcmread(r'D:\\\\Data\\\\Big Pancreas (CT, EUS)\\\\Raw Data Hospital\\\\Guilan\\\\Valid Case\\\\PG1002-malihe hoseynlo\\\\DICOMDIR')\n",
    "js=ds.to_json()\n",
    "data=json.loads(js)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydicom as pm\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def rename_duplicate_columns(df):\n",
    "    \"\"\"Rename duplicate columns in the DataFrame.\"\"\"\n",
    "    cols = pd.Series(df.columns)\n",
    "    for dup in cols[cols.duplicated()].unique(): \n",
    "        cols[cols[cols == dup].index.values.tolist()] = [dup + '_DUP' + str(i) if i != 0 else dup for i in range(sum(cols == dup))]\n",
    "    df.columns = cols\n",
    "\n",
    "Hospital_name= \"Guilan\"\n",
    "directory_shortlist=f\"D:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\{Hospital_name}_data_short_just_dcm.xlsx\"\n",
    "directory_longlist=f\"D:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\{Hospital_name}_data.csv\"\n",
    "\n",
    "\n",
    "directory_longlist=pd.read_csv(directory_longlist)\n",
    "directory_longlist_dcm=directory_longlist[directory_longlist['If_dicom']==True]\n",
    "directory_longlist=pd.read_csv(directory_longlist)\n",
    "directory_longlist_dcm=directory_longlist[directory_longlist['If_dicom']==True]\n",
    "directory_longlist_dcm=directory_longlist_dcm.reset_index()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import pydicom as pm\n",
    "\n",
    "dcminfo_list = []  # List to store the individual DataFrame pieces\n",
    "\n",
    "for i in range(1, len(directory_longlist_dcm)):\n",
    "    dir_path = os.path.join(directory_longlist_dcm.iloc[i][4], directory_longlist_dcm.iloc[i][3])\n",
    "    ds = pm.dcmread(dir_path)\n",
    "    ds = pd.DataFrame(ds.values())\n",
    "    ds[0] = ds[0].apply(lambda x: pm.dataelem.DataElement_from_raw(x) if isinstance(x, pm.dataelem.RawDataElement) else x)\n",
    "    ds['name'] = ds[0].apply(lambda x: x.name)\n",
    "    ds['value'] = ds[0].apply(lambda x: x.value)\n",
    "    ds = ds[['name', 'value']]\n",
    "    ds = ds.T\n",
    "    new_header = ds.iloc[0]  # First row as header\n",
    "    ds = ds[1:]  # Taking the rest of the data\n",
    "    ds.columns = new_header  # Setting the new header\n",
    "    ds['to_directory'] = dir_path\n",
    "    ds['key2csv']=directory_longlist_dcm['Unnamed: 0'][i]\n",
    "    \n",
    "\n",
    "    dcminfo_list.append(ds)\n",
    "\n",
    "for df in dcminfo_list:\n",
    "    rename_duplicate_columns(df)\n",
    "\n",
    "\n",
    "dcminfo_all=pd.concat(dcminfo_list, ignore_index=True, sort=False)\n",
    "dcminfo_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from previous dataframe of directories, read all dicoms.\n",
    "dcminfo_list = []  # List to store the individual DataFrame pieces\n",
    "\n",
    "print(f'total rows in your dataframe is {len(directory_longlist_dcm)}')\n",
    "start_time=time.time()\n",
    "\n",
    "for i in range(1, len(directory_longlist_dcm)):\n",
    "    dir_path = os.path.join(directory_longlist_dcm.iloc[i][4], directory_longlist_dcm.iloc[i][3])\n",
    "    ds = pm.dcmread(dir_path)\n",
    "    js=ds.to_json()\n",
    "    \n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        percentage = (i / len(directory_longlist_dcm)) * 100\n",
    "        end_time = time.time() \n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f'Processed {i} rows, which is {percentage:.2f}% of total rows in {elapsed_time} seconds.')\n",
    "\n",
    "    ds['to_directory'] = dir_path\n",
    "    ds['key2csv']=directory_longlist_dcm['Unnamed: 0'][i]\n",
    "    \n",
    "\n",
    "    dcminfo_list.append(ds)\n",
    "\n",
    "for df in dcminfo_list:\n",
    "    rename_duplicate_columns(df)\n",
    "\n",
    "\n",
    "dcminfo_all=pd.concat(dcminfo_list, ignore_index=True, sort=False)\n",
    "dcminfo_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from previous dataframe of directories, read all dicoms.\n",
    "dcminfo_list = []  # List to store the individual DataFrame pieces\n",
    "\n",
    "print(f'total rows in your dataframe is {len(directory_longlist_dcm)}')\n",
    "start_time=time.time()\n",
    "\n",
    "for i in range(1, len(directory_longlist_dcm)):\n",
    "    dir_path = os.path.join(directory_longlist_dcm.iloc[i][4], directory_longlist_dcm.iloc[i][3])\n",
    "    ds = pm.dcmread(dir_path)\n",
    "    js=ds.to_json()\n",
    "    \n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        percentage = (i / len(directory_longlist_dcm)) * 100\n",
    "        end_time = time.time() \n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f'Processed {i} rows, which is {percentage:.2f}% of total rows in {elapsed_time} seconds.')\n",
    "\n",
    "    ds['to_directory'] = dir_path\n",
    "    ds['key2csv']=directory_longlist_dcm['Unnamed: 0'][i]\n",
    "    \n",
    "\n",
    "    dcminfo_list.append(ds)\n",
    "\n",
    "for df in dcminfo_list:\n",
    "    rename_duplicate_columns(df)\n",
    "\n",
    "\n",
    "dcminfo_all=pd.concat(dcminfo_list, ignore_index=True, sort=False)\n",
    "dcminfo_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ds = pm.dcmread(r'D:\\\\Data\\\\Big Pancreas (CT, EUS)\\\\Raw Data Hospital\\\\Guilan\\\\Valid Case\\\\PG1002-malihe hoseynlo\\\\DICOMDIR')\n",
    "js=ds.to_json()\n",
    "data=json.loads(js)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to flatten the JSON recursively\n",
    "def flatten_json(y):\n",
    "    out = {}\n",
    "\n",
    "    def flatten(x, name=''):\n",
    "        if type(x) is dict:\n",
    "            for a in x:\n",
    "                flatten(x[a], name + a + '_')\n",
    "        elif type(x) is list:\n",
    "            i = 0\n",
    "            for a in x:\n",
    "                flatten(a, name + str(i) + '_')\n",
    "                i += 1\n",
    "        else:\n",
    "            out[name[:-1]] = x\n",
    "\n",
    "    flatten(y)\n",
    "    return out\n",
    "\n",
    "flat_data = flatten_json(js)\n",
    "dff = pd.DataFrame([flat_data])\n",
    "dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcminfo_all.to_excel(f\"D:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\{Hospital_name}_testdicomdataframe.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add this to the first block in your note book to show json files in the jupyter output\n",
    "import uuid\n",
    "from IPython.core.display import display, HTML\n",
    "import json\n",
    "\n",
    "class RenderJSON(object):\n",
    "    def __init__(self, json_data):\n",
    "        if isinstance(json_data, dict):\n",
    "            self.json_str = json.dumps(json_data)\n",
    "        else:\n",
    "            self.json_str = json_data\n",
    "        self.uuid = str(uuid.uuid4())\n",
    "        # This line is missed out in most of the versions of this script across the web, it is essential for this to work interleaved with print statements\n",
    "        self._ipython_display_()\n",
    "        \n",
    "    def _ipython_display_(self):\n",
    "        display(HTML('<div id=\"{}\" style=\"height: auto; width:100%;\"></div>'.format(self.uuid)))\n",
    "        display(HTML(\"\"\"<script>\n",
    "        require([\"https://rawgit.com/caldwell/renderjson/master/renderjson.js\"], function() {\n",
    "        renderjson.set_show_to_level(1)\n",
    "        document.getElementById('%s').appendChild(renderjson(%s))\n",
    "        });</script>\n",
    "        \"\"\" % (self.uuid, self.json_str)))\n",
    "\n",
    "# Since this is copy-pasted wrongly(mostly) at a lot of places across the web, i'm putting the fixed, updated version here, mainly for self-reference\n",
    "\n",
    "\n",
    "## To use this function, call this, this now works even when you have a print statement before or after the RenderJSON call\n",
    "#RenderJSON(dict_to_render)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
