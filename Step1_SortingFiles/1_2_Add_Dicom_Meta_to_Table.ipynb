{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer dicom files to pseudonymization destination\n",
    "\n",
    "<details>\n",
    "<summary>STEP 1 BIG PICTURE</summary>\n",
    "We collected data from centers in folders, named as patient ID (e.g. admission). We want to clean these directories, so \n",
    "I: Each CT study is placed in one folder\n",
    "II: Store cases in an excel file, with its dicom files in the table, and all other variables (outcome, clinical, pathology data) stored here. We call this master key, which also contains patient id (un-anonymized) along with the key for anonymization.\n",
    "III: Transfer dicom-pnly files to new destination and anonymize these images.\n",
    "</details>\n",
    "<details>\n",
    "<summary>PREVIOUS STEP</summary>\n",
    "We find all file types in our directory (I ran the code for each center sepratly. Having 1.5 terabytes of informaiton and ~1800 cases, it collectivly took 30 hours on a RTX3080Ti labtob and Corei912gen and 32Ram)\n",
    "</details>\n",
    "<details>\n",
    "<summary>THIS STEP</summary>\n",
    "In this step we will add unique dicom meta data about patient info, study info, and series info\n",
    "</details>\n",
    "<details>\n",
    "<summary>NEXT STEP</summary>\n",
    "Finding dicom meta data\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add this to the first block in your note book to show json files in the jupyter output\n",
    "import uuid\n",
    "from IPython.core.display import display, HTML\n",
    "import pydicom as pm\n",
    "import os\n",
    "\n",
    "import json\n",
    "\n",
    "class RenderJSON(object):\n",
    "    def __init__(self, json_data):\n",
    "        if isinstance(json_data, dict):\n",
    "            self.json_str = json.dumps(json_data)\n",
    "        else:\n",
    "            self.json_str = json_data\n",
    "        self.uuid = str(uuid.uuid4())\n",
    "        # This line is missed out in most of the versions of this script across the web, it is essential for this to work interleaved with print statements\n",
    "        self._ipython_display_()\n",
    "        \n",
    "    def _ipython_display_(self):\n",
    "        display(HTML('<div id=\"{}\" style=\"height: auto; width:100%;\"></div>'.format(self.uuid)))\n",
    "        display(HTML(\"\"\"<script>\n",
    "        require([\"https://rawgit.com/caldwell/renderjson/master/renderjson.js\"], function() {\n",
    "        renderjson.set_show_to_level(1)\n",
    "        document.getElementById('%s').appendChild(renderjson(%s))\n",
    "        });</script>\n",
    "        \"\"\" % (self.uuid, self.json_str)))\n",
    "\n",
    "# Since this is copy-pasted wrongly(mostly) at a lot of places across the web, i'm putting the fixed, updated version here, mainly for self-reference\n",
    "\n",
    "\n",
    "## To use this function, call this, this now works even when you have a print statement before or after the RenderJSON call\n",
    "RenderJSON(dict_to_render)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#E1 that works\n",
    "import pydicom as pm\n",
    "import os\n",
    "\n",
    "def get_dicomdir_give_dicomseriesdic_v1(dicom_dir, dicom_validation=False):\n",
    "    \"\"\"\n",
    "    This function lists all DICOM series names and numbers in a directory. \n",
    "    Set dicom_validation=True to validate DICOM files before processing.\n",
    "    Returns a dictionary with series numbers as keys and series names as values.\n",
    "    \"\"\"\n",
    "\n",
    "    series_info = {}\n",
    "    for root, dirs, files in os.walk(dicom_dir):\n",
    "        for file in files:\n",
    "            if dicom_validation:\n",
    "                if not pm.misc.is_dicom(os.path.join(root, file)):\n",
    "                    continue\n",
    "            try:\n",
    "                dicom_file = pm.dcmread(os.path.join(root, file))\n",
    "                series_name = dicom_file.get((0x0008, 0x103E), None)  # Series Description Tag\n",
    "                series_number = dicom_file.get((0x0020, 0x0011), None)  # Series Number Tag\n",
    "                if series_name and series_number:\n",
    "                    series_info[series_name.value] = series_number.value\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file}: {e}\")\n",
    "                continue                \n",
    "    return series_info\n",
    "\n",
    "#dicom_dir = r\"D:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\Taleghani\\Maybe Case Image\\243018\"\n",
    "#series = get_dicomdir_give_dicomseriesdic_v1(dicom_dir)\n",
    "#print(series)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#E2 that works (I solved many problems, such as finding the unique studies)\n",
    "\n",
    "import pydicom as pm\n",
    "import os\n",
    "\n",
    "def get_dicomdir_give_dicomseriesdic_json(dicom_dir, dicom_validation=False):\n",
    "    \"\"\"\n",
    "    This function creates a multi-level dictionary for DICOM series in a directory.\n",
    "    The top level has the last component of dicom_dir as a key, mapping to a dictionary\n",
    "    of study IDs and their corresponding series names.\n",
    "    Set dicom_validation=True to validate DICOM files before processing.\n",
    "    \"\"\"\n",
    "\n",
    "    last_dir_name = os.path.basename(os.path.normpath(dicom_dir))\n",
    "    dicom_data = {last_dir_name: {}}\n",
    "\n",
    "    for root, dirs, files in os.walk(dicom_dir):\n",
    "        for file in files:\n",
    "            if dicom_validation:\n",
    "                if not pm.misc.is_dicom(os.path.join(root, file)):\n",
    "                    continue\n",
    "            try:\n",
    "                dicom_file = pm.dcmread(os.path.join(root, file))\n",
    "                study_id = dicom_file.get((0x0020, 0x0010), None)  # Study ID Tag\n",
    "                series_name = dicom_file.get((0x0008, 0x103E), None)  # Series Description Tag\n",
    "                if study_id and series_name:\n",
    "                    study_id_value = study_id.value\n",
    "                    series_name_value = series_name.value\n",
    "                    if study_id_value not in dicom_data[last_dir_name]:\n",
    "                        dicom_data[last_dir_name][study_id_value] = []\n",
    "                    if series_name_value not in dicom_data[last_dir_name][study_id_value]:\n",
    "                        dicom_data[last_dir_name][study_id_value].append(series_name_value)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file}: {e}\")\n",
    "                continue                \n",
    "    return dicom_data\n",
    "\n",
    "dicom_dir = r\"D:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\Taleghani\\Maybe Case Image\\243018\"\n",
    "series_2=get_dicomdir_give_dicomseriesdic_json(dicom_dir)\n",
    "series_2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#E3\n",
    "import pydicom as pm\n",
    "import os\n",
    "\n",
    "def get_dicomdir_give_dicomseriesdic(dicom_dir, dicom_validation=False):\n",
    "    \"\"\"\n",
    "    This function creates a multi-level dictionary for DICOM series in a directory.\n",
    "    The top level has the last component of dicom_dir as a key, mapping to a dictionary\n",
    "    of study IDs and their corresponding series names and numbers.\n",
    "    Set dicom_validation=True to validate DICOM files before processing.\n",
    "    \"\"\"\n",
    "\n",
    "    last_dir_name = os.path.basename(os.path.normpath(dicom_dir))\n",
    "    dicom_data = {last_dir_name: {}}\n",
    "\n",
    "    for root, dirs, files in os.walk(dicom_dir):\n",
    "        for file in files:\n",
    "            if dicom_validation:\n",
    "                if not pm.misc.is_dicom(os.path.join(root, file)):\n",
    "                    continue\n",
    "            try:\n",
    "                dicom_file = pm.dcmread(os.path.join(root, file))\n",
    "                study_id = dicom_file.get((0x0020, 0x0010), None)  # Study ID Tag\n",
    "                series_name = dicom_file.get((0x0008, 0x103E), None)  # Series Description Tag\n",
    "                series_number = dicom_file.get((0x0020, 0x0011), None)  # Series Number Tag\n",
    "                subdirectory = root.split(last_dir_name)[-1]\n",
    "                if study_id and series_name and series_number:\n",
    "                    series_info = (series_number.value, series_name.value)\n",
    "                    study_info= (study_id.value, subdirectory)\n",
    "                    if study_info not in dicom_data[last_dir_name]:\n",
    "                        dicom_data[last_dir_name][study_info] = []\n",
    "                    if series_info not in dicom_data[last_dir_name][study_info]:\n",
    "                        dicom_data[last_dir_name][study_info].append(series_info)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file}: {e}\")\n",
    "                continue                \n",
    "    return dicom_data\n",
    "\n",
    "#dicom_dir = r\"D:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\Taleghani\\Maybe Case Image\\243018\"\n",
    "dicom_dir=r'D:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\Dr Radmard\\Valid Case'\n",
    "series = get_dicomdir_give_dicomseriesdic(dicom_dir,dicom_validation=True)\n",
    "series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#E4\n",
    "import pydicom as pm\n",
    "import os\n",
    "\n",
    "def generate_study_identifier(dicom_file):\n",
    "    # Use a combination of tags to create a unique identifier for each study\n",
    "    study_id = dicom_file.get((0x0020, 0x0010), \"NoID\").value  # Study ID\n",
    "    study_date = dicom_file.get((0x0008, 0x0020), \"NoDate\").value  # Study Date\n",
    "    patient_id = dicom_file.get((0x0010, 0x0020), \"NoPatientID\").value  # Patient ID\n",
    "    return f\"{study_id}_{study_date}_{patient_id}\"\n",
    "\n",
    "def get_dicomdir_give_dicomseriesdic(dicom_dir, dicom_validation=False):\n",
    "    \"\"\"\n",
    "    This function creates a multi-level dictionary for DICOM series in a directory.\n",
    "    The top level has the last component of dicom_dir as a key, mapping to a dictionary\n",
    "    of study IDs and their corresponding series names and numbers.\n",
    "    Set dicom_validation=True to validate DICOM files before processing.\n",
    "    \"\"\"\n",
    "\n",
    "    last_dir_name = os.path.basename(os.path.normpath(dicom_dir))\n",
    "    dicom_data = {last_dir_name: {}}\n",
    "\n",
    "    for root, dirs, files in os.walk(dicom_dir):\n",
    "        for file in files:\n",
    "            if dicom_validation:\n",
    "                if not pm.misc.is_dicom(os.path.join(root, file)):\n",
    "                    continue\n",
    "            try:\n",
    "                dicom_file = pm.dcmread(os.path.join(root, file))\n",
    "                study_id = dicom_file.get((0x0020, 0x0010), None)  # Study ID Tag\n",
    "                series_name = dicom_file.get((0x0008, 0x103E), None)  # Series Description Tag\n",
    "                series_number = dicom_file.get((0x0020, 0x0011), None)  # Series Number Tag\n",
    "                study_date = dicom_file.get((0x0008, 0x0020), None)\n",
    "                subdirectory = root.split(dicom_dir)[-1]\n",
    "                subdirectory = [part for part in subdirectory.split('\\\\') if part]\n",
    "                first_subfolder = subdirectory[0] if subdirectory else None\n",
    "                manufacturer= dicom_file.get((0x0008, 0x1090), None)\n",
    "                Study_description= dicom_file.get((0x0008, 0x1030), None)\n",
    "                Image_type= dicom_file.get((0x0008, 0x0008), None)\n",
    "                slice_thickness=dicom_file.get((0x0018, 0x0050), None)\n",
    "                Pt_Sex= dicom_file.get((0x0010, 0x0040), None)\n",
    "                Pt_Age= dicom_file.get((0x0010, 0x1010), None)\n",
    "                Image_comment=dicom_file.get((0x0020, 0x4000), None)\n",
    "                Protocol=dicom_file.get((0x0018, 0x1030), None)\n",
    "                body_part=dicom_file.get((0x0018, 0x0015), None)\n",
    "                if study_id and series_name and series_number and study_date:\n",
    "                    series_info = (series_number.value, series_name.value)\n",
    "                    study_unique= f'{first_subfolder}_{study_id.value}_{study_date.value}'\n",
    "                    if study_unique not in dicom_data[last_dir_name]:\n",
    "                        dicom_data[last_dir_name][study_unique] = []\n",
    "                        dicom_data[last_dir_name][study_unique]['dir_to_root']= [{root}]\n",
    "                        dicom_data[last_dir_name][study_unique]['study_description']= [{Study_description.vavlue}]\n",
    "                        dicom_data[last_dir_name][study_unique]['date']= [{study_date.value}]\n",
    "                        dicom_data[last_dir_name][study_unique]['age']= [{Pt_Age.value}]\n",
    "                        dicom_data[last_dir_name][study_unique]['sex']= [{Pt_Sex.value}]\n",
    "                        dicom_data[last_dir_name][study_unique]['manufacture']= [{manufacturer.value}]\n",
    "                        dicom_data[last_dir_name][study_unique]['protocol']= [{Protocol.value}]\n",
    "\n",
    "\n",
    "                    if series_info not in dicom_data[last_dir_name]['image_series']:\n",
    "                        dicom_data[last_dir_name][study_unique]['image_series_list'].append(series_info)\n",
    "                        dicom_data[last_dir_name][study_unique]['Image_type'].append(Image_type)\n",
    "                        dicom_data[last_dir_name][study_unique]['Image_type'].append(body_part)\n",
    "                        dicom_data[last_dir_name][study_unique]['slice_thickness'].append(slice_thickness)\n",
    "                        dicom_data[last_dir_name][study_unique]['Image_comment'].append(Image_comment)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file}: {e}\")\n",
    "                continue                \n",
    "    return dicom_data\n",
    "\n",
    "#dicom_dir=r'D:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\Dr Radmard\\Valid Case'\n",
    "dicom_dir=r\"D:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\Example\\example s\"\n",
    "series = get_dicomdir_give_dicomseriesdic(dicom_dir,dicom_validation=True)\n",
    "series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming multi-level dictionary to json\n",
    "\n",
    "series_json = json.dumps(series, indent=4)\n",
    "RenderJSON(series_json)\n",
    "\n",
    "json_file_name='dicomseries.json'\n",
    "saveto_json_dir= os.path.join(dicom_dir, json_file_name)\n",
    "with open(saveto_json_dir, 'w') as file:\n",
    "    file.write(series_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARCHIVED CODES (TRASH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data['Full_Directory'])):\n",
    "    i=0\n",
    "\n",
    "    if data['If_dicom'] is True:\n",
    "        dcm_dir=\"{}\\{}\".format(data[\"Full_Directory\"].iloc[0],data[\"File\"].iloc[0])\n",
    "        dcm_dir\n",
    "        \n",
    "    else:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydicom as pm\n",
    "\n",
    "dcm_dir=\"{}\\{}\".format(data[\"Full_Directory\"].iloc[0],data[\"File\"].iloc[0])\n",
    "dicom_file = pm.dcmread(dcm_dir)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hospital_name= \"Guilan\"\n",
    "directory_shortlist=f\"D:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\{Hospital_name}_data_short_just_dcm.xlsx\"\n",
    "directory_longlist=f\"D:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\{Hospital_name}_data.csv\"\n",
    "\n",
    "\n",
    "\n",
    "directory_longlist=pd.read_csv(directory_longlist)\n",
    "directory_longlist_dcm=directory_longlist[directory_longlist['If_dicom']==True]\n",
    "\n",
    "directory_longlist_dcm=directory_longlist[directory_longlist['If_dicom']==True]\n",
    "directory_longlist_dcm=directory_longlist_dcm.reset_index()\n",
    "\n",
    "\n",
    "\n",
    "i=1\n",
    "dir_path = os.path.join(directory_longlist_dcm.iloc[i][3], directory_longlist_dcm.iloc[i][2])\n",
    "dir_path\n",
    "\n",
    "\n",
    "\n",
    "# this code aimed to get all dicom meta data so I can work with them, and know them, especially for anonymization and knowing the phase.\n",
    "# however, it failed due to different dicom formats. I will use json instead (I should have use it at first place).\n",
    "# the json also has many erorrs, so I added the try-except into the loop to handle erorrs while finishing the loop.\n",
    "\n",
    "dcminfo_list = []  # List to store the individual DataFrame pieces\n",
    "\n",
    "print(f'total rows in your dataframe is {len(directory_longlist_dcm)}')\n",
    "start_time=time.time()\n",
    "\n",
    "for i in range(1, len(directory_longlist_dcm)):\n",
    "    dir_path = os.path.join(directory_longlist_dcm.iloc[i][3], directory_longlist_dcm.iloc[i][2])\n",
    "    \n",
    "    try:\n",
    "        ds = pm.dcmread(dir_path)\n",
    "        ds = pd.DataFrame(ds.values())\n",
    "        if ds.shape[1]>1:\n",
    "            ds= pd.DataFrame({'WARNING_MORETHAN1ROW_DF2CELL': [ds.to_string()]})\n",
    "        else: \n",
    "            ds[0] = ds[0].apply(lambda x: pm.dataelem.DataElement_from_raw(x) if isinstance(x, pm.dataelem.RawDataElement) else x)\n",
    "            ds['name'] = ds[0].apply(lambda x: x.name)\n",
    "            ds['value'] = ds[0].apply(lambda x: x.value)\n",
    "            ds = ds[['name', 'value']]\n",
    "            ds = ds.T\n",
    "            new_header = ds.iloc[0]  # First row as header\n",
    "            ds = ds[1:]  # Taking the rest of the data\n",
    "            ds.columns = new_header  # Setting the new header\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            percentage = (i / len(directory_longlist_dcm)) * 100\n",
    "            end_time = time.time() \n",
    "            elapsed_time = end_time - start_time\n",
    "            print(f'Processed {i} rows, which is {percentage:.2f}% of total rows in {elapsed_time} seconds.')\n",
    "\n",
    "        ds['to_directory'] = dir_path\n",
    "        ds['key2csv']=directory_longlist_dcm['Unnamed: 0'][i]\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_message = str(e)\n",
    "        ds = pd.DataFrame({'WARNING_ERROR': [error_message], 'to_directory': dir_path, 'key2csv': directory_longlist_dcm['Unnamed: 0'][i]})\n",
    "\n",
    "    dcminfo_list.append(ds)\n",
    "\n",
    "for df in dcminfo_list:\n",
    "    rename_duplicate_columns(df)\n",
    "\n",
    "\n",
    "dcminfo_all=pd.concat(dcminfo_list, ignore_index=True, sort=False)\n",
    "dcminfo_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ds = pm.dcmread(r'D:\\\\Data\\\\Big Pancreas (CT, EUS)\\\\Raw Data Hospital\\\\Guilan\\\\Valid Case\\\\PG1002-malihe hoseynlo\\\\DICOMDIR')\n",
    "js=ds.to_json()\n",
    "data=json.loads(js)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydicom as pm\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def rename_duplicate_columns(df):\n",
    "    \"\"\"Rename duplicate columns in the DataFrame.\"\"\"\n",
    "    cols = pd.Series(df.columns)\n",
    "    for dup in cols[cols.duplicated()].unique(): \n",
    "        cols[cols[cols == dup].index.values.tolist()] = [dup + '_DUP' + str(i) if i != 0 else dup for i in range(sum(cols == dup))]\n",
    "    df.columns = cols\n",
    "\n",
    "Hospital_name= \"Guilan\"\n",
    "directory_shortlist=f\"D:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\{Hospital_name}_data_short_just_dcm.xlsx\"\n",
    "directory_longlist=f\"D:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\{Hospital_name}_data.csv\"\n",
    "\n",
    "\n",
    "directory_longlist=pd.read_csv(directory_longlist)\n",
    "directory_longlist_dcm=directory_longlist[directory_longlist['If_dicom']==True]\n",
    "directory_longlist=pd.read_csv(directory_longlist)\n",
    "directory_longlist_dcm=directory_longlist[directory_longlist['If_dicom']==True]\n",
    "directory_longlist_dcm=directory_longlist_dcm.reset_index()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import pydicom as pm\n",
    "\n",
    "dcminfo_list = []  # List to store the individual DataFrame pieces\n",
    "\n",
    "for i in range(1, len(directory_longlist_dcm)):\n",
    "    dir_path = os.path.join(directory_longlist_dcm.iloc[i][4], directory_longlist_dcm.iloc[i][3])\n",
    "    ds = pm.dcmread(dir_path)\n",
    "    ds = pd.DataFrame(ds.values())\n",
    "    ds[0] = ds[0].apply(lambda x: pm.dataelem.DataElement_from_raw(x) if isinstance(x, pm.dataelem.RawDataElement) else x)\n",
    "    ds['name'] = ds[0].apply(lambda x: x.name)\n",
    "    ds['value'] = ds[0].apply(lambda x: x.value)\n",
    "    ds = ds[['name', 'value']]\n",
    "    ds = ds.T\n",
    "    new_header = ds.iloc[0]  # First row as header\n",
    "    ds = ds[1:]  # Taking the rest of the data\n",
    "    ds.columns = new_header  # Setting the new header\n",
    "    ds['to_directory'] = dir_path\n",
    "    ds['key2csv']=directory_longlist_dcm['Unnamed: 0'][i]\n",
    "    \n",
    "\n",
    "    dcminfo_list.append(ds)\n",
    "\n",
    "for df in dcminfo_list:\n",
    "    rename_duplicate_columns(df)\n",
    "\n",
    "\n",
    "dcminfo_all=pd.concat(dcminfo_list, ignore_index=True, sort=False)\n",
    "dcminfo_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from previous dataframe of directories, read all dicoms.\n",
    "dcminfo_list = []  # List to store the individual DataFrame pieces\n",
    "\n",
    "print(f'total rows in your dataframe is {len(directory_longlist_dcm)}')\n",
    "start_time=time.time()\n",
    "\n",
    "for i in range(1, len(directory_longlist_dcm)):\n",
    "    dir_path = os.path.join(directory_longlist_dcm.iloc[i][4], directory_longlist_dcm.iloc[i][3])\n",
    "    ds = pm.dcmread(dir_path)\n",
    "    js=ds.to_json()\n",
    "    \n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        percentage = (i / len(directory_longlist_dcm)) * 100\n",
    "        end_time = time.time() \n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f'Processed {i} rows, which is {percentage:.2f}% of total rows in {elapsed_time} seconds.')\n",
    "\n",
    "    ds['to_directory'] = dir_path\n",
    "    ds['key2csv']=directory_longlist_dcm['Unnamed: 0'][i]\n",
    "    \n",
    "\n",
    "    dcminfo_list.append(ds)\n",
    "\n",
    "for df in dcminfo_list:\n",
    "    rename_duplicate_columns(df)\n",
    "\n",
    "\n",
    "dcminfo_all=pd.concat(dcminfo_list, ignore_index=True, sort=False)\n",
    "dcminfo_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from previous dataframe of directories, read all dicoms.\n",
    "dcminfo_list = []  # List to store the individual DataFrame pieces\n",
    "\n",
    "print(f'total rows in your dataframe is {len(directory_longlist_dcm)}')\n",
    "start_time=time.time()\n",
    "\n",
    "for i in range(1, len(directory_longlist_dcm)):\n",
    "    dir_path = os.path.join(directory_longlist_dcm.iloc[i][4], directory_longlist_dcm.iloc[i][3])\n",
    "    ds = pm.dcmread(dir_path)\n",
    "    js=ds.to_json()\n",
    "    \n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        percentage = (i / len(directory_longlist_dcm)) * 100\n",
    "        end_time = time.time() \n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f'Processed {i} rows, which is {percentage:.2f}% of total rows in {elapsed_time} seconds.')\n",
    "\n",
    "    ds['to_directory'] = dir_path\n",
    "    ds['key2csv']=directory_longlist_dcm['Unnamed: 0'][i]\n",
    "    \n",
    "\n",
    "    dcminfo_list.append(ds)\n",
    "\n",
    "for df in dcminfo_list:\n",
    "    rename_duplicate_columns(df)\n",
    "\n",
    "\n",
    "dcminfo_all=pd.concat(dcminfo_list, ignore_index=True, sort=False)\n",
    "dcminfo_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ds = pm.dcmread(r'D:\\\\Data\\\\Big Pancreas (CT, EUS)\\\\Raw Data Hospital\\\\Guilan\\\\Valid Case\\\\PG1002-malihe hoseynlo\\\\DICOMDIR')\n",
    "js=ds.to_json()\n",
    "data=json.loads(js)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to flatten the JSON recursively\n",
    "def flatten_json(y):\n",
    "    out = {}\n",
    "\n",
    "    def flatten(x, name=''):\n",
    "        if type(x) is dict:\n",
    "            for a in x:\n",
    "                flatten(x[a], name + a + '_')\n",
    "        elif type(x) is list:\n",
    "            i = 0\n",
    "            for a in x:\n",
    "                flatten(a, name + str(i) + '_')\n",
    "                i += 1\n",
    "        else:\n",
    "            out[name[:-1]] = x\n",
    "\n",
    "    flatten(y)\n",
    "    return out\n",
    "\n",
    "flat_data = flatten_json(js)\n",
    "dff = pd.DataFrame([flat_data])\n",
    "dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcminfo_all.to_excel(f\"D:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\{Hospital_name}_testdicomdataframe.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
