{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Dicom meta data to MasterKey\n",
    "\n",
    "<details>\n",
    "<summary>STEP 1 BIG PICTURE</summary>\n",
    "We collected data from centers in folders, named as patient ID (e.g. admission). We want to clean these directories, so \n",
    "I: Each CT study is placed in one folder\n",
    "II: Store cases in an excel file, with its dicom files in the table, and all other variables (outcome, clinical, pathology data) stored here. We call this master key, which also contains patient id (un-anonymized) along with the key for anonymization.\n",
    "III: Transfer dicom-pnly files to new destination and anonymize these images.\n",
    "</details>\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>PREVIOUS STEP</summary>\n",
    "We find all file types in our directory (I ran the code for each center sepratly. Having 1.5 terabytes of informaiton and ~1800 cases, it collectivly took 30 hours on a RTX3080Ti labtob and Corei912gen and 32Ram)\n",
    "</details>\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>THIS STEP</summary>\n",
    "In this step we will add unique dicom meta data about patient info, study info, and series info\n",
    "</details>\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>NEXT STEP</summary>\n",
    "Finding dicom meta data\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINAL 20231216\n",
    "#My context: I coded this on my windows11 with RTC3080Ti and Corei9-12gen and 32G Ram. I am coding on VS code and using jupyter notebook.\n",
    "\n",
    "import pydicom as pm #for reading dicoms\n",
    "#Your requirment: It doesn't need any exceptional hardward you can run it on an average pc/labtob\n",
    "import os #for looping through system direcotries\n",
    "from pydicom.multival import MultiValue #for reading dicom metadata\n",
    "from pydicom.valuerep import PersonName #since tunring dictionary to json raised an error you should use this\n",
    "from tqdm.notebook import tqdm #for that fancy loop progress, I like it though\n",
    "import pandas as pd #for tunring dic to excel, first we trasnform it to pandas dataframe\n",
    "import json #for storing as json\n",
    "\n",
    "from IPython.display import HTML #so you can click on the sotred excel and json and open it from jupyter notebook\n",
    "\n",
    "def get_dicom_tag_value(dicom_file, tag, default=None):\n",
    "    '''this function will get the dicom tag from the dicom filde for the given tag/code'''\n",
    "    tag_value = dicom_file.get(tag, None)\n",
    "    if tag_value is None:\n",
    "        return default\n",
    "    if isinstance(tag_value, MultiValue):\n",
    "        return list(tag_value)  # Convert MultiValue to list\n",
    "    return tag_value.value\n",
    "\n",
    "def get_path_to_first_subfolder(full_path, first_subfolder):\n",
    "    \"\"\"this will get the path to the first folder of root, which is the subfolder that contains all dicom filed of one dicom study \"\"\"\n",
    "    path_parts = full_path.split(os.sep)\n",
    "    if first_subfolder in path_parts:\n",
    "        subfolder_index = path_parts.index(first_subfolder)\n",
    "        return os.sep.join(path_parts[:subfolder_index + 1])\n",
    "    else:\n",
    "        return full_path\n",
    "\n",
    "def count_subfolders(directory):\n",
    "    '''this will cont the number of files and folders within a direcotyr'''\n",
    "    total_subfolders = 0\n",
    "    total_files=0\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        total_subfolders += len(dirs)\n",
    "        total_files += len(files)\n",
    "    return total_subfolders,total_files \n",
    "\n",
    "\n",
    "class CustomJSONEncoder(json.JSONEncoder): #this class will turn our multilevel dictionary into a json file\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, MultiValue):\n",
    "            return list(obj)  # Convert MultiValue to list\n",
    "        elif isinstance(obj, PersonName):\n",
    "            return str(obj)   # Convert PersonName to string\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "def ensure_json_extension(directory): \n",
    "    '''this function will ensure that definied json direcotry contains the required extension, otherwise, it will add this to the end of definied dir'''\n",
    "    if not directory.endswith(\".json\"):\n",
    "        return directory + \"\\\\JSON.json\"\n",
    "    return directory\n",
    "\n",
    "def ensure_excel_extension(directory):\n",
    "    '''this function will ensure that definied excel direcotry contains the required extension, otherwise, it will add this to the end of definied dir'''\n",
    "    if not directory.endswith(\".xlsx\"):\n",
    "        return directory + \"\\\\excel.xlsx\"\n",
    "    return directory\n",
    "\n",
    "def create_clickable_dir_path(dir_path):\n",
    "    # Convert the directory path to a file URL\n",
    "    file_url = f\"{dir_path}\"\n",
    "    return HTML(f'<a href=\"{file_url}\" target=\"_blank\">{dir_path}</a>')\n",
    "\n",
    "\n",
    "\n",
    "def get_dicomdir_give_dicomdicom_datadic(dicom_dir, #direcotry that you want to read, usually dicom studies should be in one folder, preferably with patient unique id/name\n",
    "                                     dicom_validation=True, #this will check wether the file in the loop is dicom or not. Although make it slower, I recommend using it to ensure only dicom files go through loop \n",
    "                                     folder_list_name_indicomdir=None, #In your dicom_dir you can include list of folders name that you want to read. It will not read other folders. Kepp in mind that this will look into subfolders in the main folder, and not the subfolders of subfolders :)\n",
    "                                     store_as_json_dir=None, #if you want to store your ditionary as json, give your desired json direcotry\n",
    "                                     store_as_excel_dir=None #if you want to store your ditionary as excel, give your desired excel direcotry\n",
    "                                     ):\n",
    "    \"\"\"\n",
    "    This function creates a multi-level dictionary for DICOM meta data (named dicom_data) in a directory (named dicom_dir).\n",
    "    The top level has the last component of dicom_dir, which is the first level subfolder, as a key.\n",
    "    For each subforled it will store study data within this dic, along with another dicitonary for series data, within this study dictionary.\n",
    "    For series dictionary the data corresponding for series number will be stored.\n",
    "    We also have another private_info dictionary within subfodler dictionary.\n",
    "    \n",
    "    - dicom_validation: If you set dicom_validation=True, it will validate the file in the loop for being an dicom file. This is super important although it makes code slower.\n",
    "    Becaouse, sometimes some dicom files have no extension, and also reading other files may cause error in the loop.\n",
    "    \n",
    "    - folder_list_name_indicomdir: #In your dicom_dir you can include list of folders name that you want to read. It will not read other folders. Kepp in mind that this will look into subfolders in the main folder, and not the subfolders of subfolders :)\n",
    "    \n",
    "    - store_as_json_dir: if you want to store your ditionary as json, give your desired json direcotry\n",
    "    \n",
    "    - store_as_excel_dir: if you want to store your ditionary as excel, give your desired excel direcotry\n",
    "    \n",
    "    For using this function, the best practice is to place each folder containing one dicom study in subfolder, under the dicom_dir. \n",
    "    However, you can change finding unique dicom studies, even placed next to each other beacouse I definied the study_unique=f'{first_subfolder}_{study_id}_{study_date}'.\n",
    "    If you want your code to be faster you can chane the study_unique to study_unique=first_subfolder. It makes your code 15% faster, sometimes at the cost of incurrect retrival.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    total_subfolder,total_files=count_subfolders(dicom_dir)\n",
    "    print(f'your direcotry contains {total_subfolder} folders and {total_files} files')\n",
    "    \n",
    "    last_dir_name = os.path.basename(os.path.normpath(dicom_dir))\n",
    "    dicom_data = {last_dir_name: {}}\n",
    "\n",
    "    for root, dirs, files in tqdm(os.walk(dicom_dir), desc=\"Processing directories\", total=total_subfolder,unit='folder'):\n",
    "        if folder_list_name_indicomdir:\n",
    "            split_path = root.replace(dicom_dir, '').split(os.sep)\n",
    "            first_subfolder = split_path[1] if len(split_path) > 1 else \"\"\n",
    "            if first_subfolder not in folder_list_name_indicomdir:\n",
    "                print(f\"\"\"The folder {first_subfolder} was not in your definied list.\"\"\")\n",
    "                continue  # Skip if the first subfolder is not in the user-defined list\n",
    "            \n",
    "        for file in files:\n",
    "            if dicom_validation and not pm.misc.is_dicom(os.path.join(root, file)):\n",
    "                continue # Skip if the it is not dicom file\n",
    "                   \n",
    "\n",
    "            try:\n",
    "                dicom_file = pm.dcmread(os.path.join(root, file))\n",
    "                study_id = get_dicom_tag_value(dicom_file, (0x0020, 0x0010))\n",
    "                dicom_data_number = get_dicom_tag_value(dicom_file, (0x0020, 0x0011))\n",
    "                study_date = get_dicom_tag_value(dicom_file, (0x0008, 0x0020))\n",
    "                split_path = root.replace(dicom_dir, '').split(os.sep)\n",
    "                first_subfolder = split_path[1] if len(split_path) > 1 else \"\"\n",
    "                if study_id and dicom_data_number and study_date:\n",
    "                    study_unique = f'{first_subfolder}_{study_id}_{study_date}' #you can change it for increasing the speed > study_unique=first_subfolder\n",
    "                    if study_unique not in dicom_data[last_dir_name]:\n",
    "                        private_info={'name': get_dicom_tag_value(dicom_file, (0x0010, 0x0010)),\n",
    "                                      'institute': get_dicom_tag_value(dicom_file, (0x0008, 0x0080)),\n",
    "                                      'patient_id': get_dicom_tag_value(dicom_file, (0x0010, 0x0020)),\n",
    "                                      'accession_number':get_dicom_tag_value(dicom_file, (0x0008, 0x0050))\n",
    "                                      }\n",
    "                        \n",
    "                        dicom_data[last_dir_name][study_unique] = {\n",
    "                            'dir_to_root': get_path_to_first_subfolder(root, first_subfolder),\n",
    "                            'study_description': get_dicom_tag_value(dicom_file, (0x0008, 0x1030)),\n",
    "                            'date': study_date,\n",
    "                            'age': get_dicom_tag_value(dicom_file, (0x0010, 0x1010)),\n",
    "                            'sex': get_dicom_tag_value(dicom_file, (0x0010, 0x0040)),\n",
    "                            'manufacture_model': get_dicom_tag_value(dicom_file, (0x0008, 0x1090)),\n",
    "                            'manufacture_brand': get_dicom_tag_value(dicom_file, (0x0008, 0x0070)),\n",
    "                            'manufacture_brand': get_dicom_tag_value(dicom_file, (0x0008, 0x0070)),\n",
    "                            'protocol': get_dicom_tag_value(dicom_file, (0x0018, 0x1030)),\n",
    "                            'study_id': study_id,\n",
    "                            'patient_weight': get_dicom_tag_value(dicom_file, (0x0010, 0x1030)),\n",
    "                            'Image_type': get_dicom_tag_value(dicom_file, (0x0008, 0x0008)),\n",
    "                            'body_part': get_dicom_tag_value(dicom_file, (0x0018, 0x0015)),\n",
    "                            'modalitty':get_dicom_tag_value(dicom_file, (0x0008, 0x0050)),\n",
    "                            'private_info':private_info,\n",
    "                            'image_dicom_data_list': {}\n",
    "                        }\n",
    "\n",
    "                    \n",
    "\n",
    "                    dicom_data_info = {\n",
    "                        'dicom_data_description': get_dicom_tag_value(dicom_file, (0x0008, 0x103E)),\n",
    "                        'body_part': get_dicom_tag_value(dicom_file, (0x0018, 0x0015)),\n",
    "                        'slice_thickness': get_dicom_tag_value(dicom_file, (0x0018, 0x0050)),\n",
    "                        'Image_comment': get_dicom_tag_value(dicom_file, (0x0020, 0x4000)),\n",
    "                        'kvp': get_dicom_tag_value(dicom_file, (0x0018, 0x0060)),\n",
    "                        'exposure': get_dicom_tag_value(dicom_file, (0x0018, 0x1152)),\n",
    "                        'exposure_time': get_dicom_tag_value(dicom_file, (0x0018, 0x1150)),\n",
    "                    }\n",
    "                    dicom_data[last_dir_name][study_unique]['image_dicom_data_list'][dicom_data_number] = dicom_data_info\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\"\"Error reading for {file}::: {e} \\n \"\"\")\n",
    "                continue\n",
    "            \n",
    "    if store_as_json_dir is not None:\n",
    "        try:\n",
    "            json_read = json.dumps(dicom_data, indent=4, cls=CustomJSONEncoder)\n",
    "            store_as_json_dir=str(store_as_json_dir)\n",
    "            store_as_json_dir=ensure_json_extension(store_as_json_dir)\n",
    "            with open(store_as_json_dir, 'w') as json_file:\n",
    "                json_file.write(json_read)\n",
    "            print(f\"\"\"Json stored at :::\"\"\")\n",
    "            display(create_clickable_dir_path(store_as_json_dir))         \n",
    "        except:\n",
    "            print(f\"\"\"Error storing the json ::: {e} \\n \"\"\")\n",
    "            \n",
    "    if store_as_excel_dir is not None:\n",
    "        try:\n",
    "            dataframes = []\n",
    "            for key, value in dicom_data.items():\n",
    "                # Convert value to DataFrame if necessary\n",
    "                df = pd.DataFrame(value)\n",
    "                # Add the key as a new column or as part of the index\n",
    "                df['Key'] = key  # Add key as a column\n",
    "                # df = df.set_index(['Key'], append=True)  # Add key as part of a MultiIndex\n",
    "                dataframes.append(df)\n",
    "\n",
    "            # Concatenate all dataframes\n",
    "            df2 = pd.concat(dataframes).T\n",
    "            store_as_excel_dir=str(store_as_excel_dir)\n",
    "            store_as_excel_dir=ensure_excel_extension(store_as_excel_dir)\n",
    "            df2.to_excel(store_as_excel_dir)\n",
    "            print(f\"\"\"Excel stored at :::\"\"\")\n",
    "            display(create_clickable_dir_path(store_as_excel_dir))          \n",
    "        except:\n",
    "            print(f\"\"\"Error storing the excel ::: {e} \\n \"\"\")\n",
    "            \n",
    "                                 \n",
    "    return dicom_data\n",
    "\n",
    "\n",
    "#example of running code\n",
    "#dicom_dir=r\"F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\Valid Case\" \n",
    "#save_dir_json=r'F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\all_dcm.json'\n",
    "#save_dir_xlsx=r'F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\all_dcm.xlsx'\n",
    "#\n",
    "#\n",
    "#dicom_dic=get_dicomdir_give_dicomdicom_datadic(\n",
    "#    dicom_dir, #direcotry that you want to read, usually dicom studies should be in one folder, preferably with patient unique id/name\n",
    "#                                     dicom_validation=True, #this will check wether the file in the loop is dicom or not. Although make it slower, I recommend using it to ensure only dicom files go through loop \n",
    "#                                     folder_list_name_indicomdir=None, #In your dicom_dir you can include list of folders name that you want to read. It will not read other folders. Kepp in mind that this will look into subfolders in the main folder, and not the subfolders of subfolders :)\n",
    "#                                     store_as_json_dir=save_dir_json, #if you want to store your ditionary as json, give your desired json direcotry\n",
    "#                                     store_as_excel_dir=save_dir_xlsx #if you want to store your ditionary as excel, give your desired excel direcotry\n",
    "#                                     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO List\n",
    "\n",
    "# r\"F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\Shiraz\\Valid Case Image\"                   #Done  \n",
    "\n",
    "# r'F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\Taleghani\\Maybe Case Image',               #Done \n",
    "# r'F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\Taleghani\\Maybe Control Image'             #Done\n",
    "\n",
    "# r\"F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\Dr Radmard\\Valid Case\"                     #Done\n",
    "\n",
    "#r\"F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\Emam Kh\\Maybe Case Image\"                   #Done\n",
    "#r\"F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\Emam Kh\\Maybe Control (wo rep)\"             #Done\n",
    "#r\"F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\Emam Kh\\Maybe Control Image\"                #Done\n",
    "#r\"F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\Emam Kh\\Valid Case\"                         #Done\n",
    "\n",
    "#r\"F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\Guilan\\Valid Case\"                           #Done\n",
    "#r\"F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\Guilan\\Valid Control\"                        #Done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicom_dir=r\"F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\Firoozgar\\Maybe Case Image\" \n",
    "save_dir_json=r'F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\Firoozgar_MaybeCase_all_dcm.json'\n",
    "save_dir_xlsx=r'F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\Firoozgar_MaybeCase_all_dcm.xlsx'\n",
    "\n",
    "\n",
    "dicom_dic=get_dicomdir_give_dicomdicom_datadic(\n",
    "    dicom_dir, #direcotry that you want to read, usually dicom studies should be in one folder, preferably with patient unique id/name\n",
    "                                     dicom_validation=True, #this will check wether the file in the loop is dicom or not. Although make it slower, I recommend using it to ensure only dicom files go through loop \n",
    "                                     folder_list_name_indicomdir=None, #In your dicom_dir you can include list of folders name that you want to read. It will not read other folders. Kepp in mind that this will look into subfolders in the main folder, and not the subfolders of subfolders :)\n",
    "                                     store_as_json_dir=save_dir_json, #if you want to store your ditionary as json, give your desired json direcotry\n",
    "                                     store_as_excel_dir=save_dir_xlsx #if you want to store your ditionary as excel, give your desired excel direcotry\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the datasets after concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean duplicate rows in All_CT\n",
    "import ast\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "dir_to_allCTsheet=r\"F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\MasterKeyE3.xlsx\"\n",
    "dir_to_allCTsheet_sheetname='All_CT_2023'\n",
    "\n",
    "df_allCT=pd.read_excel(dir_to_allCTsheet,sheet_name=dir_to_allCTsheet_sheetname)\n",
    "df_allCT\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def merge_rows(df, group_cols, dict_col):\n",
    "    def safe_literal_eval(item):\n",
    "        try:\n",
    "            # Convert string representation of dictionary to actual dictionary\n",
    "            return ast.literal_eval(item)\n",
    "        except (ValueError, SyntaxError):\n",
    "            # If there is a parsing error, print the error and return None\n",
    "            print(f\"Failed to parse dictionary from: {item}\")\n",
    "            return None\n",
    "\n",
    "    def merge_dicts(dict_series):\n",
    "        merged_dict = {}\n",
    "        for item in dict_series:\n",
    "            d = safe_literal_eval(item) if isinstance(item, str) else item\n",
    "            if d is not None:\n",
    "                for key, value in d.items():\n",
    "                    if key not in merged_dict:\n",
    "                        merged_dict[key] = value\n",
    "                    else:\n",
    "                        # If the key is duplicate, store the values in a list\n",
    "                        if not isinstance(merged_dict[key], list):\n",
    "                            merged_dict[key] = [merged_dict[key]]\n",
    "                        merged_dict[key].append(value)\n",
    "        return merged_dict\n",
    "\n",
    "    def custom_merge(series):\n",
    "        # Filter out empty strings and NaN values, and get unique values\n",
    "        values = [str(v) for v in series if pd.notna(v) and v not in ['', 'nan', 'None']]\n",
    "        unique_values = list(pd.unique(values))\n",
    "        \n",
    "        # Join unique values with ';;' or return a single value\n",
    "        return unique_values[0] if len(unique_values) == 1 else ';;'.join(unique_values)\n",
    "\n",
    "    # Convert dir_to_root column to standard directory format\n",
    "    df['dir_to_root'] = df['dir_to_root'].apply(os.path.normpath)\n",
    "    \n",
    "    # Use custom_merge for non-dictionary columns and merge_dicts for dict_col\n",
    "    agg_dict = {col: custom_merge if col != dict_col else merge_dicts for col in df.columns if col not in group_cols}\n",
    "    \n",
    "    # Perform the groupby and aggregation\n",
    "    df_merged = df.groupby(group_cols).agg(agg_dict).reset_index()\n",
    "    \n",
    "    return df_merged\n",
    "\n",
    "\n",
    "group_columns = ['dir_to_root', 'date']\n",
    "dictionary_column = 'image_dicom_data_list'\n",
    "\n",
    "# Use the merge_rows function to merge the DataFrame\n",
    "df_allCT_merged = merge_rows(df_allCT, group_columns, dictionary_column)\n",
    "\n",
    "df_allCT_merged.to_excel(r'F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\pre_MasterKey4.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge MasterKey E3 with this All_CT\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "dir_to_allCTsheet=r\"F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\MasterKeyE3.xlsx\"\n",
    "dir_to_allCTsheet_sheetname='All_CT_2023_E1'\n",
    "\n",
    "df_allCT_E1=pd.read_excel(dir_to_allCTsheet,sheet_name=dir_to_allCTsheet_sheetname)\n",
    "\n",
    "\n",
    "dir_to_mastere3=r\"F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\MasterKeyE3.xlsx\"\n",
    "dir_to_mastere3_sheetname='MasterKey'\n",
    "\n",
    "df_masterE3=pd.read_excel(dir_to_mastere3,sheet_name=dir_to_mastere3_sheetname)\n",
    "df_masterE3 #df_allCT_E1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df_allCT_E1 and df_masterE3 are already defined\n",
    "\n",
    "# Standardize the directory paths\n",
    "df_allCT_E1['dir_to_root'] = df_allCT_E1['dir_to_root'].apply(lambda x: os.path.normpath(x) if isinstance(x, str) else x)\n",
    "df_masterE3['Directory'] = df_masterE3['Directory'].apply(lambda x: os.path.normpath(x) if isinstance(x, str) else x)\n",
    "\n",
    "\n",
    "def modify_donotpublish_id(val):\n",
    "    if isinstance(val, str) and val[:4].isdigit() and '_' in val:\n",
    "        return val.split('_')[0]\n",
    "    return val\n",
    "\n",
    "df_allCT_E1['DONOTPUBLISH_ID_fromCT'] = df_allCT_E1['DONOTPUBLISH_ID_fromCT'].apply(modify_donotpublish_id)\n",
    "\n",
    "\n",
    "# Filter out rows from df_masterE3 where 'Directory' is NaN or empty\n",
    "df_masterE3_filtered = df_masterE3.dropna(subset=['Directory'])\n",
    "df_masterE3_filtered = df_masterE3_filtered[df_masterE3_filtered['Directory'] != '']\n",
    "\n",
    "# Perform a left merge with the filtered df_masterE3\n",
    "merged_df = pd.merge(df_allCT_E1, df_masterE3_filtered, left_on='dir_to_root', right_on='Directory', how='left', indicator='based_on_dir')\n",
    "\n",
    "# Count the number of merged rows\n",
    "num_merged_rows = merged_df[merged_df['based_on_dir'] == 'both'].shape[0]\n",
    "num_keymaster_rows=df_masterE3.shape[0]\n",
    "print(\"Number of Merged Rows using same directories:\", num_merged_rows, \"from total of Rows in Keymaster: \",num_keymaster_rows )\n",
    "\n",
    "# Create a DataFrame for unmatched rows from the filtered df_masterE3\n",
    "df_remainedunmatched = df_masterE3[~df_masterE3['MasterKeyE3'].isin(merged_df['MasterKeyE3'])]\n",
    "\n",
    "print(\"Number of remaining rows that have dir but wasn't matched:\",  df_masterE3_filtered.shape[0]-num_merged_rows)\n",
    "\n",
    "\n",
    "# I want to trurn ids to intigers if possible since my previous code missed some codes (maybe due to spaces)\n",
    "merged_df['DONOTPUBLISH_ID_fromCT'] = merged_df['DONOTPUBLISH_ID_fromCT'].str.rstrip()\n",
    "\n",
    "\n",
    "merged_df['ID_HOSPITAL'] = merged_df['DONOTPUBLISH_ID_fromCT'].astype(str) + \"_\" + merged_df['Hospital'].astype(str)\n",
    "df_remainedunmatched['ID_HOSPITAL']=df_remainedunmatched['DONOTPUBLISH_ID'].astype(str) + \"_\" + df_remainedunmatched['Source'].astype(str)\n",
    "\n",
    "merged_df2 = pd.merge(merged_df, df_remainedunmatched, left_on='ID_HOSPITAL', right_on='ID_HOSPITAL', how='outer', indicator='based_on_id')\n",
    "\n",
    "# Count the number of merged rows\n",
    "num_merged_rows = merged_df2[merged_df2['based_on_id'] == 'both'].shape[0]\n",
    "num_keymaster_rows=df_masterE3.shape[0]\n",
    "Number_of_rightonly = merged_df2[merged_df2['based_on_id'] == 'right_only'].shape[0]\n",
    "print(\"Number of Merged Rows using id and hosptial:\", num_merged_rows, \"from total of Rows in Keymaster: \",num_keymaster_rows )\n",
    "print(f\"from {df_remainedunmatched.shape[0]} rows remained after removing  previously matched rows, {num_merged_rows} were merged by matching id_hospital and {Number_of_rightonly} rows were added without matching into first df (right only). \")\n",
    "\n",
    "combined_master_key = pd.concat([merged_df2['MasterKeyE3_x'].dropna(), merged_df2['MasterKeyE3_y'].dropna()])\n",
    "\n",
    "df_remainedunmatched2 = df_masterE3[~df_masterE3['MasterKeyE3'].isin(combined_master_key)]\n",
    "print(f\"The number of totally remained rows withou transfering into the final df is {df_remainedunmatched2.shape[0]} rows\")\n",
    "\n",
    "\n",
    "# Saving the merged DataFrame to an Excel file\n",
    "merged_df2.to_excel(r'F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\test2.xlsx')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Number of Merged Rows using same directories: 443 from total of Rows in Keymaster:  900\n",
    "Number of remaining rows that have dir but wasn't matched: -1\n",
    "Number of Merged Rows using id and hosptial: 311 from total of Rows in Keymaster:  900\n",
    "from 458 rows remained after removing  previously matched rows, 311 were merged by matching id_hospital and 1384 rows were added without matching into first df (right only). \n",
    "The number of totally remained rows withou transfering into the final df is 0 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Assuming df_masterE3 and df_allCT_E1 are already defined\n",
    "\n",
    "# Standardize Directory Paths\n",
    "df_masterE3['Directory'] = df_masterE3['Directory'].apply(lambda x: os.path.normpath(x) if isinstance(x, str) else x)\n",
    "df_allCT_E1['dir_to_root'] = df_allCT_E1['dir_to_root'].apply(lambda x: os.path.normpath(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Modify DONOTPUBLISH_ID_fromCT based on the specified condition\n",
    "def modify_donotpublish_id(val):\n",
    "    if isinstance(val, str) and val[:4].isdigit() and '_' in val:\n",
    "        return val.split('_')[0]\n",
    "    return val\n",
    "\n",
    "df_allCT_E1['DONOTPUBLISH_ID_fromCT'] = df_allCT_E1['DONOTPUBLISH_ID_fromCT'].apply(modify_donotpublish_id)\n",
    "\n",
    "# Merge dataframes based on Condition 1\n",
    "merged_data_1 = pd.merge(df_allCT_E1, df_masterE3[['Directory', 'DONOTPUBLISH_ID', 'Source']], left_on='dir_to_root', right_on='Directory', how='left')\n",
    "\n",
    "# Merge dataframes based on Condition 2\n",
    "merged_data_2 = pd.merge(df_allCT_E1, df_masterE3[['DONOTPUBLISH_ID', 'Source']], left_on=['DONOTPUBLISH_ID_fromCT', 'Hospital'], right_on=['DONOTPUBLISH_ID', 'Source'], how='left')\n",
    "\n",
    "# Combine results from both conditions\n",
    "merged_data = pd.concat([merged_data_1, merged_data_2], ignore_index=True)\n",
    "\n",
    "# Drop duplicates\n",
    "merged_data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Append unmatched rows from df_allCT_E1\n",
    "unmatched_rows = df_allCT_E1[~df_allCT_E1.index.isin(merged_data.index)]\n",
    "merged_data = pd.concat([merged_data, unmatched_rows], ignore_index=True)\n",
    "\n",
    "# Append unmatched rows from df_masterE3\n",
    "unmatched_rows = df_masterE3[~df_masterE3.index.isin(merged_data.index)]\n",
    "merged_data = pd.concat([merged_data, unmatched_rows], ignore_index=True)\n",
    "\n",
    "# merged_data now contains the merged data with all conditions applied\n",
    "\n",
    "# final_merged_df now contains the merged data including all non-matched rows\n",
    "merged_data.to_excel(r\"F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\draft_keye4.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.notebook import tqdm #for that fancy loop progress, I like it though\n",
    "# Assuming df_masterE3 and df_allCT_E1 are already defined\n",
    "\n",
    "# Standardize Directory Paths\n",
    "df_masterE3['Directory'] = df_masterE3['Directory'].apply(lambda x: os.path.normpath(x) if isinstance(x, str) else x)\n",
    "df_allCT_E1['dir_to_root'] = df_allCT_E1['dir_to_root'].apply(lambda x: os.path.normpath(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Modify DONOTPUBLISH_ID_fromCT based on the specified condition\n",
    "def modify_donotpublish_id(val):\n",
    "    if isinstance(val, str) and val[:4].isdigit() and '_' in val:\n",
    "        return val.split('_')[0]\n",
    "    return val\n",
    "\n",
    "df_allCT_E1['DONOTPUBLISH_ID_fromCT'] = df_allCT_E1['DONOTPUBLISH_ID_fromCT'].apply(modify_donotpublish_id)\n",
    "\n",
    "# Initialize an empty DataFrame for the merged data\n",
    "merged_data = pd.DataFrame()\n",
    "\n",
    "# Loop through each row in df_allCT_E1 for matching\n",
    "for _, row in tqdm(df_allCT_E1.iterrows()):\n",
    "    matched = False\n",
    "    for _, row_master in df_masterE3.iterrows():\n",
    "        # Check Condition 1\n",
    "        if pd.notna(row_master['Directory']) and row['dir_to_root'] == row_master['Directory']:\n",
    "            merged_row = {**row.to_dict(), **row_master.to_dict()}\n",
    "            merged_data = merged_data.append(merged_row, ignore_index=True)\n",
    "            matched = True\n",
    "            break\n",
    "        # Check Condition 2\n",
    "        elif row_master['DONOTPUBLISH_ID'] == row['DONOTPUBLISH_ID_fromCT'] and row_master['Source'] == row['Hospital']:\n",
    "            merged_row = {**row.to_dict(), **row_master.to_dict()}\n",
    "            merged_data = merged_data.append(merged_row, ignore_index=True)\n",
    "            matched = True\n",
    "            break\n",
    "    if not matched:\n",
    "        merged_data = merged_data.append(row, ignore_index=True)\n",
    "\n",
    "# Append unmatched rows from df_masterE3\n",
    "masterE3_unmatched = df_masterE3[~df_masterE3.index.isin(merged_data.index)]\n",
    "merged_data = pd.concat([merged_data, masterE3_unmatched], ignore_index=True)\n",
    "\n",
    "# merged_data now contains the merged data with all conditions applied\n",
    "\n",
    "merged_data.to_excel(r\"F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\draft_keye4.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Standardize Directory Paths\n",
    "df_masterE3['Directory'] = df_masterE3['Directory'].apply(lambda x: os.path.normpath(x) if isinstance(x, str) else x)\n",
    "df_allCT_E1['dir_to_root'] = df_allCT_E1['dir_to_root'].apply(lambda x: os.path.normpath(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Modify DONOTPUBLISH_ID_fromCT based on the specified condition\n",
    "def modify_donotpublish_id(val):\n",
    "    if isinstance(val, str) and val[:4].isdigit() and '_' in val:\n",
    "        return val.split('_')[0]\n",
    "    return val\n",
    "\n",
    "df_allCT_E1['DONOTPUBLISH_ID_fromCT'] = df_allCT_E1['DONOTPUBLISH_ID_fromCT'].apply(modify_donotpublish_id)\n",
    "\n",
    "\n",
    "merged_data = pd.merge(df_allCT_E1, df_masterE3, how='outer', left_on=['DONOTPUBLISH_ID_fromCT', 'Hospital'], right_on=['DONOTPUBLISH_ID', 'Source'], suffixes=('_fromCT', '_master'))\n",
    "merged_data.to_excel(r'F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\preE4.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df_allCT_E1 and df_masterE3 are already defined\n",
    "\n",
    "# Standardize the directory paths\n",
    "df_allCT_E1['dir_to_root'] = df_allCT_E1['dir_to_root'].apply(lambda x: os.path.normpath(x) if isinstance(x, str) else x)\n",
    "df_masterE3['Directory'] = df_masterE3['Directory'].apply(lambda x: os.path.normpath(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Perform a left merge\n",
    "merged_df = pd.merge(df_allCT_E1, df_masterE3, left_on='dir_to_root', right_on='Directory', how='left')\n",
    "\n",
    "# Create a DataFrame for unmatched rows\n",
    "df_remainedunmatched = df_masterE3[~df_masterE3['Directory'].isin(merged_df['Directory'])]\n",
    "df_remainedunmatched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARCHIVED CODES (TRASH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data['Full_Directory'])):\n",
    "    i=0\n",
    "\n",
    "    if data['If_dicom'] is True:\n",
    "        dcm_dir=\"{}\\{}\".format(data[\"Full_Directory\"].iloc[0],data[\"File\"].iloc[0])\n",
    "        dcm_dir\n",
    "        \n",
    "    else:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydicom as pm\n",
    "\n",
    "dcm_dir=\"{}\\{}\".format(data[\"Full_Directory\"].iloc[0],data[\"File\"].iloc[0])\n",
    "dicom_file = pm.dcmread(dcm_dir)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hospital_name= \"Guilan\"\n",
    "directory_shortlist=f\"D:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\{Hospital_name}_data_short_just_dcm.xlsx\"\n",
    "directory_longlist=f\"D:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\{Hospital_name}_data.csv\"\n",
    "\n",
    "\n",
    "\n",
    "directory_longlist=pd.read_csv(directory_longlist)\n",
    "directory_longlist_dcm=directory_longlist[directory_longlist['If_dicom']==True]\n",
    "\n",
    "directory_longlist_dcm=directory_longlist[directory_longlist['If_dicom']==True]\n",
    "directory_longlist_dcm=directory_longlist_dcm.reset_index()\n",
    "\n",
    "\n",
    "\n",
    "i=1\n",
    "dir_path = os.path.join(directory_longlist_dcm.iloc[i][3], directory_longlist_dcm.iloc[i][2])\n",
    "dir_path\n",
    "\n",
    "\n",
    "\n",
    "# this code aimed to get all dicom meta data so I can work with them, and know them, especially for anonymization and knowing the phase.\n",
    "# however, it failed due to different dicom formats. I will use json instead (I should have use it at first place).\n",
    "# the json also has many erorrs, so I added the try-except into the loop to handle erorrs while finishing the loop.\n",
    "\n",
    "dcminfo_list = []  # List to store the individual DataFrame pieces\n",
    "\n",
    "print(f'total rows in your dataframe is {len(directory_longlist_dcm)}')\n",
    "start_time=time.time()\n",
    "\n",
    "for i in range(1, len(directory_longlist_dcm)):\n",
    "    dir_path = os.path.join(directory_longlist_dcm.iloc[i][3], directory_longlist_dcm.iloc[i][2])\n",
    "    \n",
    "    try:\n",
    "        ds = pm.dcmread(dir_path)\n",
    "        ds = pd.DataFrame(ds.values())\n",
    "        if ds.shape[1]>1:\n",
    "            ds= pd.DataFrame({'WARNING_MORETHAN1ROW_DF2CELL': [ds.to_string()]})\n",
    "        else: \n",
    "            ds[0] = ds[0].apply(lambda x: pm.dataelem.DataElement_from_raw(x) if isinstance(x, pm.dataelem.RawDataElement) else x)\n",
    "            ds['name'] = ds[0].apply(lambda x: x.name)\n",
    "            ds['value'] = ds[0].apply(lambda x: x.value)\n",
    "            ds = ds[['name', 'value']]\n",
    "            ds = ds.T\n",
    "            new_header = ds.iloc[0]  # First row as header\n",
    "            ds = ds[1:]  # Taking the rest of the data\n",
    "            ds.columns = new_header  # Setting the new header\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            percentage = (i / len(directory_longlist_dcm)) * 100\n",
    "            end_time = time.time() \n",
    "            elapsed_time = end_time - start_time\n",
    "            print(f'Processed {i} rows, which is {percentage:.2f}% of total rows in {elapsed_time} seconds.')\n",
    "\n",
    "        ds['to_directory'] = dir_path\n",
    "        ds['key2csv']=directory_longlist_dcm['Unnamed: 0'][i]\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_message = str(e)\n",
    "        ds = pd.DataFrame({'WARNING_ERROR': [error_message], 'to_directory': dir_path, 'key2csv': directory_longlist_dcm['Unnamed: 0'][i]})\n",
    "\n",
    "    dcminfo_list.append(ds)\n",
    "\n",
    "for df in dcminfo_list:\n",
    "    rename_duplicate_columns(df)\n",
    "\n",
    "\n",
    "dcminfo_all=pd.concat(dcminfo_list, ignore_index=True, sort=False)\n",
    "dcminfo_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ds = pm.dcmread(r'D:\\\\Data\\\\Big Pancreas (CT, EUS)\\\\Raw Data Hospital\\\\Guilan\\\\Valid Case\\\\PG1002-malihe hoseynlo\\\\DICOMDIR')\n",
    "js=ds.to_json()\n",
    "data=json.loads(js)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydicom as pm\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def rename_duplicate_columns(df):\n",
    "    \"\"\"Rename duplicate columns in the DataFrame.\"\"\"\n",
    "    cols = pd.Series(df.columns)\n",
    "    for dup in cols[cols.duplicated()].unique(): \n",
    "        cols[cols[cols == dup].index.values.tolist()] = [dup + '_DUP' + str(i) if i != 0 else dup for i in range(sum(cols == dup))]\n",
    "    df.columns = cols\n",
    "\n",
    "Hospital_name= \"Guilan\"\n",
    "directory_shortlist=f\"D:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\{Hospital_name}_data_short_just_dcm.xlsx\"\n",
    "directory_longlist=f\"D:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\{Hospital_name}_data.csv\"\n",
    "\n",
    "\n",
    "directory_longlist=pd.read_csv(directory_longlist)\n",
    "directory_longlist_dcm=directory_longlist[directory_longlist['If_dicom']==True]\n",
    "directory_longlist=pd.read_csv(directory_longlist)\n",
    "directory_longlist_dcm=directory_longlist[directory_longlist['If_dicom']==True]\n",
    "directory_longlist_dcm=directory_longlist_dcm.reset_index()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import pydicom as pm\n",
    "\n",
    "dcminfo_list = []  # List to store the individual DataFrame pieces\n",
    "\n",
    "for i in range(1, len(directory_longlist_dcm)):\n",
    "    dir_path = os.path.join(directory_longlist_dcm.iloc[i][4], directory_longlist_dcm.iloc[i][3])\n",
    "    ds = pm.dcmread(dir_path)\n",
    "    ds = pd.DataFrame(ds.values())\n",
    "    ds[0] = ds[0].apply(lambda x: pm.dataelem.DataElement_from_raw(x) if isinstance(x, pm.dataelem.RawDataElement) else x)\n",
    "    ds['name'] = ds[0].apply(lambda x: x.name)\n",
    "    ds['value'] = ds[0].apply(lambda x: x.value)\n",
    "    ds = ds[['name', 'value']]\n",
    "    ds = ds.T\n",
    "    new_header = ds.iloc[0]  # First row as header\n",
    "    ds = ds[1:]  # Taking the rest of the data\n",
    "    ds.columns = new_header  # Setting the new header\n",
    "    ds['to_directory'] = dir_path\n",
    "    ds['key2csv']=directory_longlist_dcm['Unnamed: 0'][i]\n",
    "    \n",
    "\n",
    "    dcminfo_list.append(ds)\n",
    "\n",
    "for df in dcminfo_list:\n",
    "    rename_duplicate_columns(df)\n",
    "\n",
    "\n",
    "dcminfo_all=pd.concat(dcminfo_list, ignore_index=True, sort=False)\n",
    "dcminfo_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from previous dataframe of directories, read all dicoms.\n",
    "dcminfo_list = []  # List to store the individual DataFrame pieces\n",
    "\n",
    "print(f'total rows in your dataframe is {len(directory_longlist_dcm)}')\n",
    "start_time=time.time()\n",
    "\n",
    "for i in range(1, len(directory_longlist_dcm)):\n",
    "    dir_path = os.path.join(directory_longlist_dcm.iloc[i][4], directory_longlist_dcm.iloc[i][3])\n",
    "    ds = pm.dcmread(dir_path)\n",
    "    js=ds.to_json()\n",
    "    \n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        percentage = (i / len(directory_longlist_dcm)) * 100\n",
    "        end_time = time.time() \n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f'Processed {i} rows, which is {percentage:.2f}% of total rows in {elapsed_time} seconds.')\n",
    "\n",
    "    ds['to_directory'] = dir_path\n",
    "    ds['key2csv']=directory_longlist_dcm['Unnamed: 0'][i]\n",
    "    \n",
    "\n",
    "    dcminfo_list.append(ds)\n",
    "\n",
    "for df in dcminfo_list:\n",
    "    rename_duplicate_columns(df)\n",
    "\n",
    "\n",
    "dcminfo_all=pd.concat(dcminfo_list, ignore_index=True, sort=False)\n",
    "dcminfo_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from previous dataframe of directories, read all dicoms.\n",
    "dcminfo_list = []  # List to store the individual DataFrame pieces\n",
    "\n",
    "print(f'total rows in your dataframe is {len(directory_longlist_dcm)}')\n",
    "start_time=time.time()\n",
    "\n",
    "for i in range(1, len(directory_longlist_dcm)):\n",
    "    dir_path = os.path.join(directory_longlist_dcm.iloc[i][4], directory_longlist_dcm.iloc[i][3])\n",
    "    ds = pm.dcmread(dir_path)\n",
    "    js=ds.to_json()\n",
    "    \n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        percentage = (i / len(directory_longlist_dcm)) * 100\n",
    "        end_time = time.time() \n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f'Processed {i} rows, which is {percentage:.2f}% of total rows in {elapsed_time} seconds.')\n",
    "\n",
    "    ds['to_directory'] = dir_path\n",
    "    ds['key2csv']=directory_longlist_dcm['Unnamed: 0'][i]\n",
    "    \n",
    "\n",
    "    dcminfo_list.append(ds)\n",
    "\n",
    "for df in dcminfo_list:\n",
    "    rename_duplicate_columns(df)\n",
    "\n",
    "\n",
    "dcminfo_all=pd.concat(dcminfo_list, ignore_index=True, sort=False)\n",
    "dcminfo_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ds = pm.dcmread(r'D:\\\\Data\\\\Big Pancreas (CT, EUS)\\\\Raw Data Hospital\\\\Guilan\\\\Valid Case\\\\PG1002-malihe hoseynlo\\\\DICOMDIR')\n",
    "js=ds.to_json()\n",
    "data=json.loads(js)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to flatten the JSON recursively\n",
    "def flatten_json(y):\n",
    "    out = {}\n",
    "\n",
    "    def flatten(x, name=''):\n",
    "        if type(x) is dict:\n",
    "            for a in x:\n",
    "                flatten(x[a], name + a + '_')\n",
    "        elif type(x) is list:\n",
    "            i = 0\n",
    "            for a in x:\n",
    "                flatten(a, name + str(i) + '_')\n",
    "                i += 1\n",
    "        else:\n",
    "            out[name[:-1]] = x\n",
    "\n",
    "    flatten(y)\n",
    "    return out\n",
    "\n",
    "flat_data = flatten_json(js)\n",
    "dff = pd.DataFrame([flat_data])\n",
    "dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcminfo_all.to_excel(f\"D:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\{Hospital_name}_testdicomdataframe.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add this to the first block in your note book to show json files in the jupyter output\n",
    "import uuid\n",
    "from IPython.core.display import display, HTML\n",
    "import json\n",
    "\n",
    "class RenderJSON(object):\n",
    "    def __init__(self, json_data):\n",
    "        if isinstance(json_data, dict):\n",
    "            self.json_str = json.dumps(json_data)\n",
    "        else:\n",
    "            self.json_str = json_data\n",
    "        self.uuid = str(uuid.uuid4())\n",
    "        # This line is missed out in most of the versions of this script across the web, it is essential for this to work interleaved with print statements\n",
    "        self._ipython_display_()\n",
    "        \n",
    "    def _ipython_display_(self):\n",
    "        display(HTML('<div id=\"{}\" style=\"height: auto; width:100%;\"></div>'.format(self.uuid)))\n",
    "        display(HTML(\"\"\"<script>\n",
    "        require([\"https://rawgit.com/caldwell/renderjson/master/renderjson.js\"], function() {\n",
    "        renderjson.set_show_to_level(1)\n",
    "        document.getElementById('%s').appendChild(renderjson(%s))\n",
    "        });</script>\n",
    "        \"\"\" % (self.uuid, self.json_str)))\n",
    "\n",
    "# Since this is copy-pasted wrongly(mostly) at a lot of places across the web, i'm putting the fixed, updated version here, mainly for self-reference\n",
    "\n",
    "\n",
    "## To use this function, call this, this now works even when you have a print statement before or after the RenderJSON call\n",
    "#RenderJSON(dict_to_render)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from IPython.core.display import display, HTML\n",
    "import json\n",
    "import pydicom as pm\n",
    "\n",
    "class RenderJSON(object):\n",
    "    def __init__(self, json_data):\n",
    "        if isinstance(json_data, dict):\n",
    "            self.json_str = json.dumps(json_data)\n",
    "        else:\n",
    "            self.json_str = json_data\n",
    "        self.uuid = str(uuid.uuid4())\n",
    "        # This line is missed out in most of the versions of this script across the web, it is essential for this to work interleaved with print statements\n",
    "        self._ipython_display_()\n",
    "        \n",
    "    def _ipython_display_(self):\n",
    "        display(HTML('<div id=\"{}\" style=\"height: auto; width:100%;\"></div>'.format(self.uuid)))\n",
    "        display(HTML(\"\"\"<script>\n",
    "        require([\"https://rawgit.com/caldwell/renderjson/master/renderjson.js\"], function() {\n",
    "        renderjson.set_show_to_level(1)\n",
    "        document.getElementById('%s').appendChild(renderjson(%s))\n",
    "        });</script>\n",
    "        \"\"\" % (self.uuid, self.json_str)))\n",
    "\n",
    "\n",
    "import pydicom as pm\n",
    "dir_to_dicom=r\"F:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\Example\\123_CT_1.dcm\"\n",
    "dicom=pm.read_file(dir_to_dicom)\n",
    "for elem in dicom.iterall():\n",
    "    print(elem)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
