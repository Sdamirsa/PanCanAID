{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer dicom files to pseudonymization destination\n",
    "\n",
    "<details>\n",
    "<summary>STEP 1 BIG PICTURE</summary>\n",
    "We collected data from centers in folders, named as patient ID (e.g. admission). We want to clean these directories, so \n",
    "I: Each CT study is placed in one folder\n",
    "II: Store cases in an excel file, with its dicom files in the table, and all other variables (outcome, clinical, pathology data) stored here. We call this master key, which also contains patient id (un-anonymized) along with the key for anonymization.\n",
    "III: Transfer dicom-pnly files to new destination and anonymize these images.\n",
    "</details>\n",
    "<details>\n",
    "<summary>PREVIOUS STEP</summary>\n",
    "WE previously asked centers to give us their data:\n",
    "1- Patients CT scan, each patient in a folder, with their admission ID.\n",
    "2- CT scan report + Pathology report (if avaialble) stored as pdf or image in the folder.\n",
    "3- We collected eligble cases in a folder (Maybe Case) and eligible controls in (Maybe control); if the data collection was physian-based or pathology-based we asked them to store these valdiated patients as (Valid Case) or (Valid control).\n",
    "4- We collected all folders from different centers (which was not that much clean, and I can totally understand the hardship of data collection for our team members). \n",
    "5- We validated cases, by reviweing their data case-by-case, and god knows how much time I put on this. If you are reading this, let me say it clearly that you should dedicate a lot of time if you want to validate cases, and if you aim not to enroll elgibile only. In my case, I lost ~50% of cases during validaiton, due to lack of relibale validation, lack of needed CT scan (e.g. post surgery or post chemo CT).\n",
    "</details>\n",
    "<details>\n",
    "<summary>THIS STEP</summary>\n",
    "This code will find all files in a direcotry, with the number of dicom files, and return a long list, containing a row for each file, and a short list, reporting the number of file types with same extension in a folder.\n",
    "\n",
    "[20231001]v1: The first final code\n",
    "[20231201]v2: I added find dicom since some dicom files has no extension (while they usually should have .dcm extension) using pydicom library. This step makes the function three times slower :) \n",
    "</details>\n",
    "<details>\n",
    "<summary>NEXT STEP</summary>\n",
    "Finding dicom meta data\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enviroment and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def get_filepaths_dataframe(directory, Ignore=None,give_second_dupreomved_df=False, multiple_or_no_dot_handler=True, find_dicom=False):\n",
    "    \"\"\"\n",
    "    List all files in a directory (file name, file extension, folder direcotry, to_file directory) while optionally ignoring specific file extensions.\n",
    "    It also checks for validity of dicom files, since some dicom files don't have any extension (they usualy have .dcm, however).\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        directory (str): The path to the directory to search for files.\n",
    "        Ignore (list, optional): A list of file extensions to ignore. If specified, files with these extensions\n",
    "        will be excluded from the list. Default is None.\n",
    "        give_second_dupreomved_df (bool, optional): If True, a second DataFrame is generated with duplicated file formats\n",
    "        within one directory and their count in the data directory.\n",
    "        multiple_or_no_dot_handler (bool, optional): If True, it will handle the file names with multiple dots, \n",
    "        or files that have no format (e.g \".dcm)\")\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame with columns 'Full_Directory' and 'File' containing file information.\n",
    "\n",
    "    If `give_second_dupreomved_df` is set to True, the function returns a tuple of two DataFrames:\n",
    "    1. The first DataFrame contains all file information, including the file's full directory, file name, and file format.\n",
    "    2. The second DataFrame (only if `give_second_dupreomved_df` is True) contains the same file information but with\n",
    "       duplicated file formats within a directory removed, along with a count of each file format in the directory.\n",
    "    \n",
    "    If `find_dicom` is set to True, the function returns a tuple of two DataFrames, including a column checking for dicom format of each file:\n",
    "    \n",
    "    \n",
    "    The function also measures the execution time and prints it to the console.\n",
    "\n",
    "    Example:\n",
    "        directory_path = r'/path/to/directory'\n",
    "        Ignore = [\".dcm\"]  # List of file extensions to ignore\n",
    "        data, count_perDirectandType = get_filepaths_dataframe(directory_path, Ignore, give_second_dupreomved_df=True, multiple_or_no_dot_handler=True,find_dicom=False)\n",
    "        print(data)  # Display all files\n",
    "        print(count_perDirectandType)  # Display files with duplicated formats removed and counts.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import os\n",
    "        import pandas as pd\n",
    "        import time\n",
    "        if find_dicom==True:\n",
    "            import pydicom\n",
    "    except ImportError:\n",
    "        raise ImportError(\"The required packages (os, pandas, time. Or pydicom if you set check dicom as true) are not imported. Please make sure to import these packages before using this function.\")\n",
    "    start_time=time.time()\n",
    "\n",
    "    if find_dicom is True:\n",
    "        data = {'File': [], 'Full_Directory': [], 'If_dicom':[]}\n",
    "        i=0        \n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                file_extension = os.path.splitext(file)[1]\n",
    "                if Ignore and file_extension in Ignore:\n",
    "                    continue  # Skip files with extensions specified in the Ignore list\n",
    "                \n",
    "                data['Full_Directory'].append(root)\n",
    "                data['File'].append(file)\n",
    "                data['If_dicom'].append(pydicom.misc.is_dicom(file_path))\n",
    "                i=i+1\n",
    "                if i%100 ==0:\n",
    "                    print(f\"{len(data['Full_Directory'])} directories extracted\")\n",
    "\n",
    "\n",
    "        \n",
    "        tmp_data=pd.DataFrame(data)     \n",
    "        data_split=tmp_data['Full_Directory'].str.split('\\\\\\\\', expand=True)\n",
    "        data_split.columns = [f'Sub_dir_{i+1}' for i in range(data_split.shape[1])]\n",
    "        data=pd.concat([tmp_data, data_split], axis=1)\n",
    "\n",
    "        if multiple_or_no_dot_handler == True:\n",
    "            data['File_Format'] = data['File'].apply(lambda x: x.rsplit('.', 1)[-1] if '.' in x else 'WARNING: NODATAFORMAT')\n",
    "        else:\n",
    "            data['File_Format'] = data['File'].str.split('.').str[-1]\n",
    "\n",
    "        if give_second_dupreomved_df==True:\n",
    "            count = data.groupby(['Full_Directory', 'File_Format','If_dicom'])['File_Format'].count().reset_index(name='Count')    \n",
    "            cdata_split=count['Full_Directory'].str.split('\\\\\\\\', expand=True)\n",
    "            cdata_split.columns = [f'Sub_dir_{i+1}' for i in range(cdata_split.shape[1])]\n",
    "            count_perDirectandType=pd.concat([count, cdata_split], axis=1)\n",
    "\n",
    "\n",
    "        end_time = time.time()  # Record the end time\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Execution time: {elapsed_time} seconds\")\n",
    "        \n",
    "\n",
    "        if give_second_dupreomved_df == True:\n",
    "            return data, count_perDirectandType\n",
    "            print(\"since you turned give_second_dupreomved_df on, this function will give you two dataframes (dupremoved with counts as the second df)\")\n",
    "        else:\n",
    "            return data\n",
    "        \n",
    "\n",
    "\n",
    "    else:\n",
    "        data = {'File': [], 'Full_Directory': []         \n",
    "                ,'to_file':[], 'If_dicom':[]}\n",
    "        \n",
    "        i=0\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                file_extension = os.path.splitext(file)[1]\n",
    "                if Ignore and file_extension in Ignore:\n",
    "                    continue  # Skip files with extensions specified in the Ignore list\n",
    "                \n",
    "                data['Full_Directory'].append(root)\n",
    "                data['File'].append(file)\n",
    "                data['If_dicom'].append(pydicom.misc.is_dicom(file_path))\n",
    "                i=i+1\n",
    "                if i%100 ==0:\n",
    "                    print(f\"{len(data['Full_Directory'])} directories extracted\")\n",
    "\n",
    "        \n",
    "        tmp_data=pd.DataFrame(data)     \n",
    "        data_split=tmp_data['Full_Directory'].str.split('\\\\\\\\', expand=True)\n",
    "        data_split.columns = [f'Sub_dir_{i+1}' for i in range(data_split.shape[1])]\n",
    "        data=pd.concat([tmp_data, data_split], axis=1)\n",
    "\n",
    "        if multiple_or_no_dot_handler == True:\n",
    "            data['File_Format'] = data['File'].apply(lambda x: x.rsplit('.', 1)[-1] if '.' in x else 'WARNING: NODATAFORMAT')\n",
    "        else:\n",
    "            data['File_Format'] = data['File'].str.split('.').str[-1]\n",
    "\n",
    "        if give_second_dupreomved_df==True:\n",
    "            count = data.groupby(['Full_Directory', 'File_Format'])['File_Format'].count().reset_index(name='Count')    \n",
    "            cdata_split=count['Full_Directory'].str.split('\\\\\\\\', expand=True)\n",
    "            cdata_split.columns = [f'Sub_dir_{i+1}' for i in range(cdata_split.shape[1])]\n",
    "            count_perDirectandType=pd.concat([count, cdata_split], axis=1)\n",
    "\n",
    "\n",
    "        end_time = time.time()  # Record the end time\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Execution time: {elapsed_time} seconds\")\n",
    "        \n",
    "\n",
    "        if give_second_dupreomved_df == True:\n",
    "            return data, count_perDirectandType\n",
    "            print(\"since you turned give_second_dupreomved_df on, this function will give you two dataframes (dupremoved with counts as the second df)\")\n",
    "        else:\n",
    "            return data\n",
    "        \n",
    "\n",
    "#\n",
    "#### F O R    D E B U G\"\"\"\"\"\n",
    "##Hospital_name= \"Guilan\"\n",
    "##directory=f\"D:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\{Hospital_name}\" #direcotry on old pc\n",
    "#directory=f\"E:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\{Hospital_name}\" #directory on ssd hard\n",
    "#\n",
    "#\n",
    "#Ignore=None\n",
    "#give_second_dupreomved_df=True\n",
    "#multiple_or_no_dot_handler=True\n",
    "#find_dicom=True\n",
    "#import os\n",
    "#import pandas as pd\n",
    "#import time\n",
    "#import pydicom\n",
    "#\n",
    "#start_time=time.time()\n",
    "#\n",
    "#data = {'File': [], 'Full_Directory': [],'If_dicom':[]}\n",
    "#\n",
    "#for root, dirs, files in os.walk(directory):\n",
    "#    for file in files:\n",
    "#        file_path = os.path.join(root, file)\n",
    "#        file_extension = os.path.splitext(file)[1]\n",
    "#        \n",
    "#        if Ignore and file_extension in Ignore:\n",
    "#            continue  # Skip files with extensions specified in the Ignore list\n",
    "#        \n",
    "#        data['Full_Directory'].append(root)\n",
    "#        data['File'].append(file)\n",
    "#        data['If_dicom'].append(pydicom.misc.is_dicom(file_path))\n",
    "#\n",
    "#\n",
    "#tmp_data=pd.DataFrame(data)   \n",
    "#print(tmp_data)\n",
    "#\n",
    "#data_split=tmp_data['Full_Directory'].str.split('\\\\\\\\', expand=True)\n",
    "#data_split.columns = [f'Sub_dir_{i+1}' for i in range(data_split.shape[1])]\n",
    "#data=pd.concat([tmp_data, data_split], axis=1)\n",
    "#\n",
    "#if multiple_or_no_dot_handler == True:\n",
    "#    data['File_Format'] = data['File'].apply(lambda x: x.rsplit('.', 1)[-1] if '.' in x else 'WARNING: NODATAFORMAT')\n",
    "#else:\n",
    "#    data['File_Format'] = data['File'].str.split('.').str[-1]\n",
    "#\n",
    "#if give_second_dupreomved_df==True:\n",
    "#    count_perDirectandType = data.groupby(['Full_Directory', 'File_Format'])['File_Format'].count().reset_index(name='Count')\n",
    "#\n",
    "#end_time = time.time()  # Record the end time\n",
    "#elapsed_time = end_time - start_time\n",
    "#print(f\"Execution time: {elapsed_time} seconds\")\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changables (for reuse) & Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changables: the variables, direcotry, file names for saving/loading should be definied here\n",
    "\n",
    "Hospital_name= \"Guilan\"\n",
    "#directory=f\"D:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\{Hospital_name}\" #directory on old pc\n",
    "directory=f\"E:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\{Hospital_name}\" #directory on ssd hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from myfunc_directory import get_filepaths_dataframe #get_filepaths_dataframe(directory, Ignore=None,give_second_dupreomved_df=False,, multiple_or_no_dot_handler=True, find_dicom=False) \n",
    "\n",
    "ignore=[] # if you want to ignore some specific file types, insert them in this list.\n",
    "data, count_perDirectandType=get_filepaths_dataframe(directory, Ignore=ignore,give_second_dupreomved_df=True, multiple_or_no_dot_handler=True,find_dicom=True)\n",
    "\n",
    "#data.to_csv(f\"D:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\{Hospital_name}_data.csv\")\n",
    "data.to_csv(f\"E:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\{Hospital_name}_data.csv\")\n",
    "#count_perDirectandType.to_excel(f\"D:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\{Hospital_name}_data_short.xlsx\")\n",
    "count_perDirectandType.to_excel(f\"E:\\Data\\Big Pancreas (CT, EUS)\\Raw Data Hospital\\{Hospital_name}_data_short.xlsx\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
